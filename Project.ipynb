{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joelsebzzz/Question-and-generation-system/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PZqCcWvv8Xp",
        "outputId": "1955a609-3bbc-442d-d173-dfd36f62d078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate datasets nltk transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15S3vuYl1wby",
        "outputId": "5293e494-b2bd-45d0-9d07-f0559ccc0df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMrvlww5ulwA",
        "outputId": "cc93f329-2a24-413c-bb05-c42e17cd78bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration for SHORT Q&A:\n",
            "  Model Checkpoint: t5-small\n",
            "  Training CSV: /content/drive/MyDrive/Testing/New/short_train.csv\n",
            "  Validation CSV: /content/drive/MyDrive/Testing/New/short_val.csv\n",
            "  Output Directory (for checkpoints & logs): /content/drive/MyDrive/Testing/New/model_outputs/short_qna_finetuned\n",
            "  Max Input Length: 750\n",
            "  Max Target Length: 128\n",
            "  Batch Size: 12\n",
            "  Epochs: 6\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import evaluate\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "\n",
        "model_checkpoint = \"t5-small\"\n",
        "\n",
        "short_qna_train_path = \"/content/drive/MyDrive/Testing/New/short_train.csv\"\n",
        "short_qna_val_path   = \"/content/drive/MyDrive/Testing/New/short_val.csv\"\n",
        "\n",
        "base_drive_output_path = \"/content/drive/MyDrive/Testing/New/model_outputs\"\n",
        "\n",
        "short_qna_output_dir = os.path.join(base_drive_output_path, \"short_qna_finetuned\")\n",
        "\n",
        "# Preprocessing Parameters\n",
        "max_input_length = 750\n",
        "max_target_length = 128\n",
        "input_prefix = \"generate question and answer: context: \"\n",
        "output_structure = \"question: {} answer: {}\"\n",
        "\n",
        "# Training Parameters\n",
        "batch_size = 12\n",
        "learning_rate = 5e-5\n",
        "num_train_epochs = 6\n",
        "weight_decay = 0.01\n",
        "logging_steps = 100\n",
        "save_steps_ratio = 0.2\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    print(\"Downloading NLTK punkt tokenizer...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "if not os.path.exists(base_drive_output_path):\n",
        "    os.makedirs(base_drive_output_path)\n",
        "    print(f\"Created base output directory: {base_drive_output_path}\")\n",
        "\n",
        "print(f\"Configuration for SHORT Q&A:\")\n",
        "print(f\"  Model Checkpoint: {model_checkpoint}\")\n",
        "print(f\"  Training CSV: {short_qna_train_path}\")\n",
        "print(f\"  Validation CSV: {short_qna_val_path}\")\n",
        "print(f\"  Output Directory (for checkpoints & logs): {short_qna_output_dir}\")\n",
        "print(f\"  Max Input Length: {max_input_length}\")\n",
        "print(f\"  Max Target Length: {max_target_length}\")\n",
        "print(f\"  Batch Size: {batch_size}\")\n",
        "print(f\"  Epochs: {num_train_epochs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo0t3tnBvIm3",
        "outputId": "3d6175a2-52cb-4c05-c6c9-8ecf0f162396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading custom short Q&A datasets from CSV using pandas...\n",
            "Successfully loaded datasets using pandas.\n",
            "\n",
            "Dataset structure:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['context_id', 'question_id', 'context', 'question', 'answer'],\n",
            "        num_rows: 2394\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['context_id', 'question_id', 'context', 'question', 'answer'],\n",
            "        num_rows: 587\n",
            "    })\n",
            "})\n",
            "\n",
            "Sample training example:\n",
            "{'context_id': 'C001', 'question_id': 'C001_Q1', 'context': 'Dynamic typing checks types at runtime. Functional programming emphasizes pure functions and immutability. Object-oriented programming organizes code into classes and objects. Static typing enforces type rules at compile time. Interpreted languages are executed line by line by an interpreter. Programming languages provide syntax and semantics to write software programs. Memory management can be manual or automatic. High-level languages are easier for humans to read and write.', 'question': 'Describe functional programming.', 'answer': 'Functional programming emphasizes pure functions, avoiding side effects and promoting immutability.'}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Check if files exist before loading\n",
        "if not os.path.exists(short_qna_train_path):\n",
        "    raise FileNotFoundError(f\"Training file not found: {short_qna_train_path}\")\n",
        "if not os.path.exists(short_qna_val_path):\n",
        "    raise FileNotFoundError(f\"Validation file not found: {short_qna_val_path}\")\n",
        "\n",
        "print(\"Loading custom short Q&A datasets from CSV using pandas...\")\n",
        "\n",
        "try:\n",
        "    # Load CSVs using pandas\n",
        "    train_df_short = pd.read_csv(short_qna_train_path)\n",
        "    val_df_short = pd.read_csv(short_qna_val_path)\n",
        "\n",
        "    # Convert pandas DataFrames to datasets.Dataset objects\n",
        "    train_dataset_short = Dataset.from_pandas(train_df_short)\n",
        "    val_dataset_short = Dataset.from_pandas(val_df_short)\n",
        "\n",
        "    # Create a DatasetDict\n",
        "    raw_datasets = DatasetDict({\n",
        "        \"train\": train_dataset_short,\n",
        "        \"validation\": val_dataset_short\n",
        "    })\n",
        "    print(\"Successfully loaded datasets using pandas.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSVs with pandas: {e}\")\n",
        "    print(\"Please ensure your CSV files are correctly formatted and paths are correct.\")\n",
        "    raise\n",
        "\n",
        "print(\"\\nDataset structure:\")\n",
        "print(raw_datasets)\n",
        "print(\"\\nSample training example:\")\n",
        "if len(raw_datasets[\"train\"]) > 0:\n",
        "    print(raw_datasets[\"train\"][0])\n",
        "else:\n",
        "    print(\"Warning: Short Q&A training dataset is empty.\")\n",
        "\n",
        "\n",
        "required_columns = ['context', 'question', 'answer']\n",
        "for split in raw_datasets.keys():\n",
        "    if len(raw_datasets[split]) > 0:\n",
        "        for col in required_columns:\n",
        "            if col not in raw_datasets[split].column_names:\n",
        "                stripped_column_names = [c.strip() for c in raw_datasets[split].column_names]\n",
        "                if col not in stripped_column_names:\n",
        "                    raise ValueError(\n",
        "                        f\"Missing required column '{col}' in '{split}' split. \"\n",
        "                        f\"Available columns: {raw_datasets[split].column_names}\"\n",
        "                    )\n",
        "    else:\n",
        "        print(f\"Warning: Short Q&A '{split}' split is empty. Skipping column check.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527,
          "referenced_widgets": [
            "0a820ef391d34fcb8dcfee3088e82883",
            "1de8b49b8f55484fba3517973556ef9d",
            "7d7c5c16b0e4460c9c79138124e1a133",
            "6f655f6df065482c893eae3e808ba49d",
            "976627ab2fab444680626b61e633631b",
            "23e9f9078b864e31a17913100f2c2a3c",
            "3587453e3a1c44d1a9f96b91f9e662a0",
            "f72882b66eb94d44a1240cfd5a082ead",
            "feacbf9718d04deea65ce483c7b73313",
            "c071e9979be64a1096cd413a05436a42",
            "4f942e39605a41eb91812700377a7453",
            "ea6b9d9a1a6642f7bea46d0f82a43bf2",
            "276173b7e4f74cdc817d93ec2c645dcc",
            "cebebd6b6b514bf7a873de56f15bbbe9",
            "4eff33247549424f910000f4e36815fd",
            "f84e1de437e844fda23a263aaeebc5bc",
            "3835051c52424e2fb56024a810ffba4b",
            "e481f94cb5a748eb918649e90d58e551",
            "017a5ef2a93f457a890f416cbd7a19a3",
            "8d2cbc0a75cb4829ac897395f8d21c29",
            "461c141a38e54eecbb7d9d12c2d0f405",
            "39fe474bdc404ac68ccc9258772ae154",
            "9721a5b313924ca7981a80fc125e4b5d",
            "0ac18e271a324b5fbffd8bb07ac5c904",
            "70a8ccb685b545489ba0b9d4efe48206",
            "936e813dd3e0453f80d290d6b054cd72",
            "5f25d0a8accb41d1a0f0afe3aed8b5ea",
            "73eb0c718b97402e9c96e5eec12b6d72",
            "a7a9da46caa5490aa97c3777131eb396",
            "cd040891b64046cb9c8d7f8596ee74c9",
            "2c9fc0fac02e4527a11b40f62184556e",
            "415b7bbc42354d14938de3c0ccbf60a4",
            "391d67bb59754806b222a8e489a1f24f",
            "211d25f6a7e14c55a0a2865c4151d1e5",
            "c2a1fd03d11c4b87becc4d86a8095360",
            "372b801bcfb64ec29eae1fe24a8fd506",
            "5046fb44028a46ffa7613e5f10395ad1",
            "68943090b2ba4edf967033fddbcbe5d2",
            "afb9d58e654d484f97c0ec72c162fb43",
            "c1fa103e12684cf78a13b9696dd4231d",
            "9277507201a446dfaebe045a38e1003e",
            "67c97cd555844661970124bbb7542f96",
            "aac881c7e0544954a20d768c7596bd90",
            "644b4fbf682d4b3f9ff154a01f30ac1e",
            "b3a701397af44d60b8932f40b99c0e35",
            "20b0336c6f0445f09d1e0d6d8a1a5277",
            "3642ce5a0b9f481bb3cc9b73692ee58c",
            "d647235d518d4e028d5ef037384c6741",
            "dad0a09daa4d410da5d1d5c2f7effbcd",
            "405aee25802f427db2d1ecd34f7594de",
            "a81c6cc028264553a601a182b59dab9a",
            "aed787c70cf644e5bba50555a9b0f266",
            "84e6007ae3d44f3c8ebbaab82af657dd",
            "c24bf4e5421a40a4adf68a049f3f6451",
            "286e3f1cf09d496c81457b21645f3b00"
          ]
        },
        "id": "ZWVFz1vp5vI9",
        "outputId": "a9f50c97-069f-4883-cc7e-78d9350a7aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading tokenizer for t5-small...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a820ef391d34fcb8dcfee3088e82883"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea6b9d9a1a6642f7bea46d0f82a43bf2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9721a5b313924ca7981a80fc125e4b5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Applying preprocessing to the datasets...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2394 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "211d25f6a7e14c55a0a2865c4151d1e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/587 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3a701397af44d60b8932f40b99c0e35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing finished.\n",
            "\n",
            "Sample Processed Input (decoded):\n",
            "generate question and answer: context: Dynamic typing checks types at runtime. Functional programming emphasizes pure functions and immutability. Object-oriented programming organizes code into classes and objects. Static typing enforces type rules at compile time. Interpreted languages are executed line by line by an interpreter. Programming languages provide syntax and semantics to write software programs. Memory management can be manual or automatic. High-level languages are easier for humans to read and write.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "\n",
            "Sample Processed Label (decoded):\n",
            "question: Describe functional programming. answer: Functional programming emphasizes pure functions, avoiding side effects and promoting immutability.</s>\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nLoading tokenizer for {model_checkpoint}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "def preprocess_short_qna(examples):\n",
        "    \"\"\"Prepares custom Q&A data for T5 fine-tuning.\"\"\"\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    contexts = examples.get('context', [])\n",
        "    questions = examples.get('question', [])\n",
        "    answers = examples.get('answer', [])\n",
        "\n",
        "    if not (len(contexts) == len(questions) == len(answers)):\n",
        "         print(f\"Warning: Mismatch in lengths: contexts ({len(contexts)}), questions ({len(questions)}), answers ({len(answers)})\")\n",
        "         min_len = min(len(contexts), len(questions), len(answers))\n",
        "         contexts, questions, answers = contexts[:min_len], questions[:min_len], answers[:min_len]\n",
        "\n",
        "\n",
        "    for context, question, answer in zip(contexts, questions, answers):\n",
        "        if not all(isinstance(item, str) for item in [context, question, answer]):\n",
        "            print(f\"Warning: Skipping record due to non-string data: Context type {type(context)}, Q type {type(question)}, A type {type(answer)}\")\n",
        "            continue # Skip this record\n",
        "\n",
        "        model_input_text = f\"{input_prefix}{context.strip()}\"\n",
        "        inputs.append(model_input_text)\n",
        "\n",
        "        model_target_text = output_structure.format(question.strip(), answer.strip())\n",
        "        targets.append(model_target_text)\n",
        "\n",
        "    # Tokenize Inputs\n",
        "    model_inputs = tokenizer(inputs,\n",
        "                             max_length=max_input_length,\n",
        "                             padding=\"max_length\",\n",
        "                             truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets,\n",
        "                           max_length=max_target_length,\n",
        "                           padding=\"max_length\",\n",
        "                           truncation=True)\n",
        "\n",
        "    label_pad_token_id = -100\n",
        "    padded_labels = []\n",
        "    for label_ids in labels[\"input_ids\"]:\n",
        "         padded_labels.append([\n",
        "             (l if l != tokenizer.pad_token_id else label_pad_token_id) for l in label_ids\n",
        "         ])\n",
        "    model_inputs[\"labels\"] = padded_labels\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "print(\"\\nApplying preprocessing to the datasets...\")\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    preprocess_short_qna,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names\n",
        ")\n",
        "print(\"Preprocessing finished.\")\n",
        "\n",
        "\n",
        "print(\"\\nSample Processed Input (decoded):\")\n",
        "print(tokenizer.decode(tokenized_datasets['train'][0]['input_ids'], skip_special_tokens=False))\n",
        "print(\"\\nSample Processed Label (decoded):\")\n",
        "label_ids_short = [id for id in tokenized_datasets['train'][0]['labels'] if id != -100]\n",
        "print(tokenizer.decode(label_ids_short, skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "9d55a6f7001547caae5c4423bb823e52",
            "3f81759bbbb4421ca4cf8f69c36712e8",
            "19003340b3cc48c190613475033c8965",
            "7e7c69c9cdb643b5a734c0545e63c1fd",
            "261211f961124c3caa97418cfc44d4bd",
            "f35f73706d744c35b1ccb58f287b07a2",
            "f63acf2c9823455d9f97f4d1132fbcdb",
            "d73820eecdef40d295437eb995dcdf8f",
            "d7a5f3509b354566a68711603fc961f4",
            "ad6761d6913a45cb8a065ec367dfce52",
            "bab8b5085f3548b7804f3148ba3b435e",
            "4a1a6d5e23e342e9b1684bf2da5243f4",
            "60e88e5ff9ad42e79753c04bb13d5dce",
            "aa63f30e24c34c90ad6a653fd961a1f8",
            "b9732fa1fd09418eba5bf68b35316cf4",
            "6a31dd30a89e4d78beda8ee464c9afaf",
            "59caef1e93d247a5812f605ebb3dea17",
            "87507ff3023040a88ad46525f971bba7",
            "d8d4465442d2433788f558db1c1dc0b3",
            "cba2c00ff9eb418887eab4ae31a4e02a",
            "7711440ddf0e4d62bf5bf3f8b887e6c0",
            "8e2e6509120d4c5a90b534374a001584",
            "060f7d7a130c469293f84cfb61d72961",
            "a11c64e927434b7f9f45bdc60864f21d",
            "fd7ae45f0e4c407bb12ed1f52c9a3f3c",
            "6287af75745a4cef9c4cd81de69acb36",
            "970fa21bf8c9454bb54cb99d74f2f586",
            "2eb7429af3e840fa9fe15ad8f48df7ef",
            "c783533698514bb6a504bf76c4c93841",
            "3da3c371751e4bbbb37d93bccb25a603",
            "454a39fb5255429ab32a33d29819a620",
            "6517d4fd19a846d58498c5c57e390744",
            "992ea64b1d654907b108b66e248f82bc"
          ]
        },
        "id": "X5y3OdQ557ex",
        "outputId": "ff841beb-b447-4133-d7b7-82712afed1ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading base model 't5-small' for Short Q&A training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d55a6f7001547caae5c4423bb823e52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a1a6d5e23e342e9b1684bf2da5243f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "060f7d7a130c469293f84cfb61d72961"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model loaded.\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nLoading base model '{model_checkpoint}' for Short Q&A training...\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "print(\"Base model loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7BUkNY-6DRw",
        "outputId": "524959fe-4bcb-4b70-fdbb-24978bc5676b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Setting up Training Arguments. Output dir: /content/drive/MyDrive/Testing/New/model_outputs/short_qna_finetuned\n",
            "Setting up Data Collator...\n",
            "Setup complete.\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nSetting up Training Arguments. Output dir: {short_qna_output_dir}\")\n",
        "\n",
        "train_dataset_size = len(tokenized_datasets[\"train\"])\n",
        "num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
        "if num_gpus == 0: num_gpus = 1\n",
        "\n",
        "steps_per_epoch = (train_dataset_size // (batch_size * num_gpus)) +1\n",
        "save_steps = int(steps_per_epoch * save_steps_ratio)\n",
        "if save_steps < 10: save_steps = logging_steps\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=short_qna_output_dir,\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size * 2,\n",
        "    weight_decay=weight_decay,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    predict_with_generate=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_dir=os.path.join(short_qna_output_dir, \"logs\"),\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=logging_steps,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    report_to=\"tensorboard\",\n",
        "    generation_max_length=max_target_length\n",
        ")\n",
        "\n",
        "print(\"Setting up Data Collator...\")\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "print(\"Setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pJwWbVG8NFH",
        "outputId": "fa48c6a8-8cd5-42de-cb39-1309e80e3e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=9ff73934ac4c056de757b1047df6abf23d36cf3133833b1ef5086aae99debbc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "b11f72533983469d85b8a1e99951df04",
            "76abbc86af484289bb3699f5ba43034b",
            "892dd14f958e408599fefb4bee1a4c25",
            "8a1c5cf8afc84514997daee88a32db25",
            "4ef069925de54f4eaf19cccc712fde06",
            "d1f9a2586c49427a9546930d16b87d24",
            "93e22f5bf6094242aa68f43072c934a2",
            "674dfaf2d3554bac8563e8f85d3756c2",
            "e96c91f7a93149b4a33dc0bac6dab9b4",
            "7b51b93bad774a14a59d07ea83f59ddd",
            "5529ad81a6f64452aab94fb77602b6c6"
          ]
        },
        "id": "5gmWZSKi6IYt",
        "outputId": "ce6d2348-5698-4be3-dbbe-d0f8ee94c2e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Setting up evaluation metrics (ROUGE)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b11f72533983469d85b8a1e99951df04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics setup complete.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSetting up evaluation metrics (ROUGE)...\")\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics_short_qna(eval_pred): # Renaming to avoid conflict if you define another later\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.where(predictions == -100, tokenizer.pad_token_id, predictions)\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds_nltk = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels_nltk = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    rouge_result = rouge_metric.compute(predictions=decoded_preds_nltk, references=decoded_labels_nltk, use_stemmer=True)\n",
        "    rouge_result = {key: value * 100 for key, value in rouge_result.items()}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    metrics = {**rouge_result, \"gen_len\": np.mean(prediction_lens)}\n",
        "\n",
        "    return {k: round(v, 4) for k, v in metrics.items()}\n",
        "\n",
        "print(\"Metrics setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "vgEr0Dv_75Mb",
        "outputId": "a6c13f69-4764-4752-f1c2-a3c5cae4d13e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "<ipython-input-11-85ff406db551>:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Setting up Trainer for Short Q&A task...\n",
            "\n",
            "Starting training for Short Q&A Generation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-85ff406db551>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting training for Short Q&A Generation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"An error occurred during training: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3789\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3793\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab') # Ensure this is available for the metrics function\n",
        "\n",
        "print(\"\\nSetting up Trainer for Short Q&A task...\")\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics_short_qna\n",
        ")\n",
        "\n",
        "print(\"\\nStarting training for Short Q&A Generation...\")\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during training: {e}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Attempting to clear CUDA cache...\")\n",
        "        torch.cuda.empty_cache()\n",
        "    raise e\n",
        "\n",
        "print(\"\\nTraining finished.\")\n",
        "\n",
        "# === PATH CORRECTION FOR SAVING BEST MODEL ===\n",
        "# Save the final model (which is the best if load_best_model_at_end=True)\n",
        "# to the main output directory for this model.\n",
        "trainer.save_model() # Saves to args.output_dir (short_qna_output_dir)\n",
        "print(f\"Final Short Q&A model (potentially best) saved to {short_qna_output_dir}\")\n",
        "\n",
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train_short\", metrics) # Differentiate metrics log\n",
        "trainer.save_metrics(\"train_short\", metrics) # Differentiate metrics file\n",
        "trainer.save_state()\n",
        "\n",
        "# Also explicitly save the best model to a dedicated 'best_model' subfolder\n",
        "# This makes loading for inference cleaner.\n",
        "short_qna_best_model_path = os.path.join(short_qna_output_dir, \"best_model\")\n",
        "if not os.path.exists(short_qna_best_model_path):\n",
        "    os.makedirs(short_qna_best_model_path)\n",
        "trainer.save_model(short_qna_best_model_path)\n",
        "print(f\"Best Short Q&A Model explicitly saved to {short_qna_best_model_path}\")\n",
        "# === PATH CORRECTION END ===\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    del model\n",
        "    if 'trainer' in globals(): del trainer # trainer might not be defined if training failed early\n",
        "    torch.cuda.empty_cache()\n",
        "print(\"Training process complete for Short Q&A.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDGFu3gY8dEL",
        "outputId": "e53d9bde-cfdc-400e-d1c3-a3735a497fd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading fine-tuned Short Q&A model and tokenizer from: /content/drive/MyDrive/Testing/New/model_outputs/short_qna_finetuned/best_model\n",
            "Short Q&A Inference model moved to device: cpu\n",
            "\n",
            "--- Example Short Q&A Inference ---\n",
            "Input Passage:\n",
            "\n",
            "The first computer-like device was created in 1822\n",
            "by Charles Babbage, an English mathematician\n",
            "and inventor. Babbage's machine, known as the\n",
            "\"Difference Engine,\" was designed to calculate\n",
            "mathematic...\n",
            "\n",
            "Generating short question and answer...\n",
            "Raw generated output: question: What is the purpose of the \"Difference Engine\"? answer: The first computer-like device was created by Charles Babbage, an English mathematician and inventor.\n",
            "Parsing successful.\n",
            "\n",
            "Generated Short Q&A Pair:\n",
            "  Q: What is the purpose of the \"Difference Engine\"?\n",
            "  A: The first computer-like device was created by Charles Babbage, an English mathematician and inventor.\n",
            "\n",
            "--- Short Q&A Inference Cell Complete ---\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import re\n",
        "import os # Ensure os is imported\n",
        "\n",
        "# --- Configuration for Short Q&A Inference ---\n",
        "# === PATH CORRECTION START ===\n",
        "# short_qna_output_dir should be defined from Cell 3.\n",
        "# If running this cell independently, redefine short_qna_output_dir or base_drive_output_path:\n",
        "# base_drive_output_path = \"/content/drive/MyDrive/Testing/New/model_outputs\"\n",
        "# short_qna_output_dir = os.path.join(base_drive_output_path, \"short_qna_finetuned\")\n",
        "\n",
        "short_qna_model_load_path = os.path.join(short_qna_output_dir, \"best_model\")\n",
        "# === PATH CORRECTION END ===\n",
        "\n",
        "model_checkpoint_inf = \"t5-small\" # Should match the trained model base\n",
        "max_input_length_inf = 750\n",
        "input_prefix_inf = \"generate question and answer: context: \"\n",
        "inf_max_output_length = 128\n",
        "inf_num_beams = 4\n",
        "inf_early_stopping = True\n",
        "inf_no_repeat_ngram_size = 2\n",
        "# ---\n",
        "\n",
        "if not os.path.exists(short_qna_model_load_path):\n",
        "    raise FileNotFoundError(f\"Fine-tuned Short Q&A model directory not found: {short_qna_model_load_path}. Please ensure training completed and saved the model to this specific path.\")\n",
        "\n",
        "print(f\"Loading fine-tuned Short Q&A model and tokenizer from: {short_qna_model_load_path}\")\n",
        "tokenizer_inf_short = AutoTokenizer.from_pretrained(short_qna_model_load_path)\n",
        "model_inf_short = AutoModelForSeq2SeqLM.from_pretrained(short_qna_model_load_path)\n",
        "\n",
        "device_inf_short = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use a distinct device var\n",
        "model_inf_short.to(device_inf_short)\n",
        "model_inf_short.eval()\n",
        "print(f\"Short Q&A Inference model moved to device: {device_inf_short}\")\n",
        "\n",
        "def generate_short_qna(context):\n",
        "    if not context or not isinstance(context, str):\n",
        "        print(\"Error: Invalid context provided.\")\n",
        "        return None\n",
        "    input_text = f\"{input_prefix_inf}{context.strip()}\"\n",
        "    inputs = tokenizer_inf_short(input_text,\n",
        "                                 max_length=max_input_length_inf,\n",
        "                                 padding=True,\n",
        "                                 truncation=True,\n",
        "                                 return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(device_inf_short)\n",
        "    attention_mask = inputs.attention_mask.to(device_inf_short)\n",
        "\n",
        "    print(f\"\\nGenerating short question and answer...\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model_inf_short.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=inf_max_output_length,\n",
        "            num_beams=inf_num_beams,\n",
        "            early_stopping=inf_early_stopping,\n",
        "            no_repeat_ngram_size=inf_no_repeat_ngram_size,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "    generated_text = tokenizer_inf_short.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    print(f\"Raw generated output: {generated_text}\")\n",
        "\n",
        "    match = re.match(r\"question:\\s*(.*?)\\s*answer:\\s*(.*)\", generated_text, re.IGNORECASE | re.DOTALL)\n",
        "    if match:\n",
        "        question = match.group(1).strip()\n",
        "        answer = match.group(2).strip()\n",
        "        print(\"Parsing successful.\")\n",
        "        return {\"question\": question, \"answer\": answer}\n",
        "    else:\n",
        "        print(\"Error: Could not parse the generated output.\")\n",
        "        parts = generated_text.lower().split('answer:', 1)\n",
        "        if len(parts) == 2:\n",
        "             q_part = parts[0].replace('question:', '').strip()\n",
        "             a_part = parts[1].strip()\n",
        "             if q_part and a_part:\n",
        "                 print(\"Fallback parsing attempted.\")\n",
        "                 return {\"question\": q_part, \"answer\": a_part}\n",
        "        return None\n",
        "\n",
        "passage_example = \"\"\"\n",
        "The first computer-like device was created in 1822\n",
        "by Charles Babbage, an English mathematician\n",
        "and inventor. Babbage's machine, known as the\n",
        "\"Difference Engine,\" was designed to calculate\n",
        "mathematical tables automatically. Although the\n",
        "device was never completed, it laid the foundation for the development of future\n",
        "computing machines.\n",
        "\"\"\"\n",
        "print(\"\\n--- Example Short Q&A Inference ---\")\n",
        "print(f\"Input Passage:\\n{passage_example[:200]}...\")\n",
        "qna_pair = generate_short_qna(passage_example)\n",
        "if qna_pair:\n",
        "    print(\"\\nGenerated Short Q&A Pair:\")\n",
        "    print(f\"  Q: {qna_pair['question']}\")\n",
        "    print(f\"  A: {qna_pair['answer']}\")\n",
        "else:\n",
        "    print(\"\\nFailed to generate a valid short Q&A pair.\")\n",
        "print(\"\\n--- Short Q&A Inference Cell Complete ---\")\n",
        "\n",
        "# Optional: Clean up\n",
        "# if torch.cuda.is_available():\n",
        "#     if 'model_inf_short' in globals(): del model_inf_short\n",
        "#     if 'tokenizer_inf_short' in globals(): del tokenizer_inf_short\n",
        "#     torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDc3Rl01alVl",
        "outputId": "f7d313b3-389e-48c8-e3dd-af58d9ab8bd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCi4-V4gZ4M0",
        "outputId": "7381ad2f-48e3-4a1f-dedd-a83d74ecbe88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration for LONG Q&A:\n",
            "  Model Checkpoint: t5-small\n",
            "  Training CSV: /content/drive/MyDrive/Testing/New/long_train.csv\n",
            "  Validation CSV: /content/drive/MyDrive/Testing/New/long_val.csv\n",
            "  Output Directory (for checkpoints & logs): /content/drive/MyDrive/Testing/New/model_outputs/long_qna_finetuned\n",
            "  Max Input Length: 750\n",
            "  Max Target Length: 256\n",
            "  Batch Size: 8\n",
            "  Epochs: 6\n"
          ]
        }
      ],
      "source": [
        "import os # Ensure os is imported if this cell is run standalone\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import evaluate\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "\n",
        "# --- Configuration for LONG Q&A ---\\\n",
        "model_checkpoint_long = \"t5-small\" # Can be same as short, or different if needed\n",
        "\n",
        "# Dataset Paths\n",
        "long_qna_train_path = \"/content/drive/MyDrive/Testing/New/long_train.csv\"\n",
        "long_qna_val_path   = \"/content/drive/MyDrive/Testing/New/long_val.csv\"\n",
        "\n",
        "# === PATH CORRECTION START ===\n",
        "# Base output directory on Drive (should be the same as defined in Cell 3 for consistency)\n",
        "# If Cell 3 was not run in this session, define it here:\n",
        "if 'base_drive_output_path' not in globals():\n",
        "    base_drive_output_path = \"/content/drive/MyDrive/Testing/New/model_outputs\"\n",
        "    if not os.path.exists(base_drive_output_path):\n",
        "        os.makedirs(base_drive_output_path)\n",
        "        print(f\"Created base output directory: {base_drive_output_path}\")\n",
        "\n",
        "\n",
        "# Output Directory for the fine-tuned LONG Q&A model\n",
        "long_qna_output_dir = os.path.join(base_drive_output_path, \"long_qna_finetuned\")\n",
        "# === PATH CORRECTION END ===\n",
        "\n",
        "# Preprocessing Parameters\n",
        "max_input_length_long = 750\n",
        "max_target_length_long = 256 # Increased for long answers\n",
        "input_prefix_long = \"generate question and answer: context: \"\n",
        "output_structure_long = \"question: {} answer: {}\"\n",
        "\n",
        "# Training Parameters\n",
        "batch_size_long = 8 # Adjusted from your original code for long Q&A\n",
        "learning_rate_long = 5e-5\n",
        "num_train_epochs_long = 6\n",
        "weight_decay_long = 0.01\n",
        "logging_steps_long = 100\n",
        "save_steps_ratio_long = 0.2\n",
        "# --- End Configuration ---\n",
        "\n",
        "# NLTK data (punkt likely already downloaded)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab') # For metrics function later\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "\n",
        "print(f\"Configuration for LONG Q&A:\")\n",
        "print(f\"  Model Checkpoint: {model_checkpoint_long}\")\n",
        "print(f\"  Training CSV: {long_qna_train_path}\")\n",
        "print(f\"  Validation CSV: {long_qna_val_path}\")\n",
        "print(f\"  Output Directory (for checkpoints & logs): {long_qna_output_dir}\") # Updated\n",
        "print(f\"  Max Input Length: {max_input_length_long}\")\n",
        "print(f\"  Max Target Length: {max_target_length_long}\")\n",
        "print(f\"  Batch Size: {batch_size_long}\")\n",
        "print(f\"  Epochs: {num_train_epochs_long}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfd8SkJOdYZ-",
        "outputId": "599044c3-26f0-48c1-a46a-e71616fb63b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading custom LONG Q&A datasets from CSV using pandas...\n",
            "Successfully loaded LONG Q&A datasets using pandas.\n",
            "\n",
            "Long Q&A Dataset structure:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['context_id', 'question_id', 'context', 'question', 'answer'],\n",
            "        num_rows: 2413\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['context_id', 'question_id', 'context', 'question', 'answer'],\n",
            "        num_rows: 578\n",
            "    })\n",
            "})\n",
            "\n",
            "Sample LONG Q&A training example:\n",
            "{'context_id': 'C001', 'question_id': 'C001_Q1', 'context': 'A central challenge in Machine Learning is balancing computational efficiency with accuracy.\\nKey applications of Machine Learning include real-world problem solving and data analysis.\\nCore theoretical concepts in Machine Learning are essential for designing efficient systems.\\nMachine Learning often relies on mathematical models and statistical methods for analysis.\\nRecent research in Machine Learning has led to significant improvements in performance and scalability.\\nGraduate-level research in Machine Learning explores novel techniques and deep learning.\\nThe concept of Machine Learning refers to the fundamental principles and techniques used in this area.\\nMachine Learning is interconnected with other fields, such as data structures and algorithms.\\nUnderstanding the history and evolution of Machine Learning provides insight into current methodologies.', 'question': 'What is the definition of Machine Learning?', 'answer': 'Students often engage with both theoretical frameworks and practical implementations to understand Machine Learning. Machine Learning is defined as the study and application of core principles and techniques that facilitate solutions. Key challenges include maintaining a balance between computational efficiency and accuracy. Hands-on projects and case studies help solidify understanding of Machine Learning.'}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd # Make sure pandas is imported\n",
        "from datasets import Dataset, DatasetDict # Make sure Dataset and DatasetDict are imported\n",
        "\n",
        "# Check if files exist before loading\n",
        "if not os.path.exists(long_qna_train_path): # Uses long_qna_train_path from Cell 13\n",
        "    raise FileNotFoundError(f\"Training file not found: {long_qna_train_path}\")\n",
        "if not os.path.exists(long_qna_val_path):   # Uses long_qna_val_path from Cell 13\n",
        "    raise FileNotFoundError(f\"Validation file not found: {long_qna_val_path}\")\n",
        "\n",
        "print(\"Loading custom LONG Q&A datasets from CSV using pandas...\")\n",
        "\n",
        "try:\n",
        "    # Load CSVs using pandas\n",
        "    train_df_long = pd.read_csv(long_qna_train_path)\n",
        "    val_df_long = pd.read_csv(long_qna_val_path)\n",
        "\n",
        "    # Convert pandas DataFrames to datasets.Dataset objects\n",
        "    train_dataset_long = Dataset.from_pandas(train_df_long)\n",
        "    val_dataset_long = Dataset.from_pandas(val_df_long)\n",
        "\n",
        "    # Create a DatasetDict, assign to raw_datasets_long\n",
        "    raw_datasets_long = DatasetDict({ # Use the correct variable name\n",
        "        \"train\": train_dataset_long,\n",
        "        \"validation\": val_dataset_long\n",
        "    })\n",
        "    print(\"Successfully loaded LONG Q&A datasets using pandas.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading LONG Q&A CSVs with pandas: {e}\")\n",
        "    print(\"Please ensure your CSV files are correctly formatted and paths are correct.\")\n",
        "    raise\n",
        "\n",
        "\n",
        "# Optional: Inspect the loaded data\n",
        "print(\"\\nLong Q&A Dataset structure:\")\n",
        "print(raw_datasets_long)\n",
        "print(\"\\nSample LONG Q&A training example:\")\n",
        "if len(raw_datasets_long[\"train\"]) > 0:\n",
        "    print(raw_datasets_long[\"train\"][0])\n",
        "else:\n",
        "    print(\"Warning: Long Q&A training dataset is empty.\")\n",
        "\n",
        "# Define required columns (can reuse variable from short Q&A section if in same scope)\n",
        "required_columns_long = ['context', 'question', 'answer']\n",
        "# Check if all required columns are present\n",
        "for split in raw_datasets_long.keys():\n",
        "    if len(raw_datasets_long[split]) > 0:\n",
        "      for col in required_columns_long:\n",
        "          if col not in raw_datasets_long[split].column_names:\n",
        "              stripped_column_names_long = [c.strip() for c in raw_datasets_long[split].column_names]\n",
        "              if col not in stripped_column_names_long:\n",
        "                  raise ValueError(\n",
        "                      f\"Missing required column '{col}' in LONG Q&A '{split}' split. \"\n",
        "                      f\"Available columns: {raw_datasets_long[split].column_names}\"\n",
        "                  )\n",
        "    else:\n",
        "        print(f\"Warning: Long Q&A '{split}' split is empty. Skipping column check.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "c13c93a95b1341c8b32a0cff935dcb94",
            "22b479b3a9c24c819266a4ba61ae5b3c",
            "3f4e161a42574dd78522128eed1e8caf",
            "722d6f09645a45aa8d5d8ed9afc3cee1",
            "4fb759d6f98449cd8837b0ca34462b8b",
            "dc1c041b24a344969be871f424146bd0",
            "80377b10514c4208a3e09f772e53147e",
            "dcfa9d1a92ce45e2bf4bc5691b8d1455",
            "4c8595f2c0814ff2bccc7655d403c756",
            "ef077c9583e2467ca94207c7619b13b7",
            "a306084eec5e409fb6e13146a2fabff4",
            "061b792354d04c42b34d080d86e3c4ae",
            "701c82ade9144b2ab71aaaef0ba0cbdc",
            "65f7ef776d6f4bf9b767f600e4ae4744",
            "8e7ee6dc4d7845ce9713844fe8a31240",
            "352c708a458e4bc3a00e3db564856799",
            "c537a4eac3e243b98b286ebd8e2f86e3",
            "877428dd8381472f9a46b67c23c09d7e",
            "5d03cb1e6dcc4c99987092a9df2658f5",
            "9ef8d9fae221439aa32a604904d8b2e9",
            "8596f8bbc80b48c3a79742852a35c511",
            "a4bc0ceb54634d1b81adf07164032189"
          ]
        },
        "id": "QNhqyhfZdeuj",
        "outputId": "9d96f44a-61f8-49db-d028-cf988c1c5e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reusing tokenizer from t5-small for LONG Q&A (or loading if not present).\n",
            "\n",
            "Applying preprocessing to the LONG Q&A datasets...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2413 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c13c93a95b1341c8b32a0cff935dcb94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/578 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "061b792354d04c42b34d080d86e3c4ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing finished for LONG Q&A.\n",
            "\n",
            "Sample Processed LONG Q&A Input (decoded):\n",
            "generate question and answer: context: A central challenge in Machine Learning is balancing computational efficiency with accuracy. Key applications of Machine Learning include real-world problem solving and data analysis. Core theoretical concepts in Machine Learning are essential for designing efficient systems. Machine Learning often relies on mathematical models and statistical methods for analysis. Recent research in Machine Learning has led to significant improvements in performance and scalability. Graduate-level research in Machine Learning explores novel techniques and deep learning. The concept of Machine Learning refers to the fundamental principles and techniques used in this area. Machine Learning is interconnected with other fields, such as data structures and algorithms. Understanding the history and evolution of Machine Learning provides insight into current methodologies.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "\n",
            "Sample Processed LONG Q&A Label (decoded):\n",
            "question: What is the definition of Machine Learning? answer: Students often engage with both theoretical frameworks and practical implementations to understand Machine Learning. Machine Learning is defined as the study and application of core principles and techniques that facilitate solutions. Key challenges include maintaining a balance between computational efficiency and accuracy. Hands-on projects and case studies help solidify understanding of Machine Learning.</s>\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nReusing tokenizer from {model_checkpoint_long} for LONG Q&A (or loading if not present).\")\n",
        "# Ensure tokenizer is available (it should be from short Q&A if run sequentially)\n",
        "if 'tokenizer' not in globals():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_long)\n",
        "\n",
        "\n",
        "def preprocess_long_qna(examples):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    contexts = examples.get('context', [])\n",
        "    questions = examples.get('question', [])\n",
        "    answers = examples.get('answer', [])\n",
        "\n",
        "    if not (len(contexts) == len(questions) == len(answers)):\n",
        "         print(f\"Warning (Long Q&A): Mismatch in lengths.\")\n",
        "         min_len = min(len(contexts), len(questions), len(answers))\n",
        "         contexts, questions, answers = contexts[:min_len], questions[:min_len], answers[:min_len]\n",
        "\n",
        "    for context, question, answer in zip(contexts, questions, answers):\n",
        "        if not all(isinstance(item, str) for item in [context, question, answer]):\n",
        "            print(f\"Warning (Long Q&A): Skipping record due to non-string data.\")\n",
        "            continue\n",
        "        model_input_text = f\"{input_prefix_long}{context.strip()}\" # Using long_qna specific prefix\n",
        "        inputs.append(model_input_text)\n",
        "        model_target_text = output_structure_long.format(question.strip(), answer.strip()) # Using long_qna specific structure\n",
        "        targets.append(model_target_text)\n",
        "\n",
        "    model_inputs = tokenizer(inputs,\n",
        "                             max_length=max_input_length_long, # Uses long_qna specific length\n",
        "                             padding=\"max_length\",\n",
        "                             truncation=True)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets,\n",
        "                           max_length=max_target_length_long, # Uses long_qna specific target length\n",
        "                           padding=\"max_length\",\n",
        "                           truncation=True)\n",
        "    label_pad_token_id = -100\n",
        "    padded_labels = []\n",
        "    for label_ids in labels[\"input_ids\"]:\n",
        "         padded_labels.append([\n",
        "             (l if l != tokenizer.pad_token_id else label_pad_token_id) for l in label_ids\n",
        "         ])\n",
        "    model_inputs[\"labels\"] = padded_labels\n",
        "    return model_inputs\n",
        "\n",
        "print(\"\\nApplying preprocessing to the LONG Q&A datasets...\")\n",
        "tokenized_datasets_long = raw_datasets_long.map(\n",
        "    preprocess_long_qna,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets_long[\"train\"].column_names\n",
        ")\n",
        "print(\"Preprocessing finished for LONG Q&A.\")\n",
        "\n",
        "if len(tokenized_datasets_long['train']) > 0:\n",
        "    print(\"\\nSample Processed LONG Q&A Input (decoded):\")\n",
        "    print(tokenizer.decode(tokenized_datasets_long['train'][0]['input_ids'], skip_special_tokens=False))\n",
        "    print(\"\\nSample Processed LONG Q&A Label (decoded):\")\n",
        "    label_ids_long_inspect = [id for id in tokenized_datasets_long['train'][0]['labels'] if id != -100]\n",
        "    print(tokenizer.decode(label_ids_long_inspect, skip_special_tokens=False))\n",
        "else:\n",
        "    print(\"Tokenized long Q&A training dataset is empty, skipping sample inspection.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv8CxiHWeHPW",
        "outputId": "5145b09a-8031-4c16-b53a-98fda17e9b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading base model 't5-small' for LONG Q&A training...\n",
            "Base model for LONG Q&A loaded.\n",
            "Deleting previous 'model' (short Q&A base) instance...\n",
            "Deleting previous 'model_inf_short' instance...\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nLoading base model '{model_checkpoint_long}' for LONG Q&A training...\")\n",
        "model_long = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint_long)\n",
        "print(\"Base model for LONG Q&A loaded.\")\n",
        "\n",
        "# Clean up previous models to free GPU memory if they exist and are different\n",
        "if 'model' in globals() and model is not model_long:\n",
        "    print(\"Deleting previous 'model' (short Q&A base) instance...\")\n",
        "    del model\n",
        "if 'model_inf_short' in globals() and model_inf_short is not model_long: # Should have been deleted earlier\n",
        "    print(\"Deleting previous 'model_inf_short' instance...\")\n",
        "    del model_inf_short\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvm9xq9oeKwd",
        "outputId": "2d454193-3373-4c53-bd25-0e9382bb6b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Setting up Training Arguments for LONG Q&A. Output dir: /content/drive/MyDrive/Testing/New/model_outputs/long_qna_finetuned\n",
            "Setting up Data Collator for LONG Q&A...\n",
            "Setup complete for LONG Q&A.\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nSetting up Training Arguments for LONG Q&A. Output dir: {long_qna_output_dir}\")\n",
        "\n",
        "train_dataset_size_long = len(tokenized_datasets_long[\"train\"])\n",
        "num_gpus_long = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
        "if num_gpus_long == 0: num_gpus_long = 1\n",
        "\n",
        "steps_per_epoch_long = (train_dataset_size_long // (batch_size_long * num_gpus_long)) + 1\n",
        "save_steps_long = int(steps_per_epoch_long * save_steps_ratio_long) # use _long suffixed var\n",
        "if save_steps_long < 10: save_steps_long = logging_steps_long # use _long suffixed var\n",
        "\n",
        "args_long = Seq2SeqTrainingArguments(\n",
        "    output_dir=long_qna_output_dir, # Corrected output dir\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=learning_rate_long,\n",
        "    per_device_train_batch_size=batch_size_long,\n",
        "    per_device_eval_batch_size=batch_size_long * 2,\n",
        "    weight_decay=weight_decay_long,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=num_train_epochs_long,\n",
        "    predict_with_generate=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_dir=os.path.join(long_qna_output_dir, \"logs_long\"), # Differentiated log dir\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=logging_steps_long,\n",
        "    save_strategy=\"epoch\",\n",
        "    # save_steps=save_steps_long, # save_strategy=\"epoch\" saves per epoch\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    report_to=\"tensorboard\",\n",
        "    generation_max_length=max_target_length_long # For eval\n",
        ")\n",
        "\n",
        "print(\"Setting up Data Collator for LONG Q&A...\")\n",
        "# Pass model_long to the collator\n",
        "data_collator_long = DataCollatorForSeq2Seq(tokenizer, model=model_long)\n",
        "print(\"Setup complete for LONG Q&A.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy7yZUxxeQ_V",
        "outputId": "3ac96b77-0b35-47ee-871d-cfebb8fae3e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Setting up Trainer for LONG Q&A task...\n",
            "Trainer for LONG Q&A setup complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-b5b5de153c78>:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_long = Seq2SeqTrainer(\n"
          ]
        }
      ],
      "source": [
        "# The compute_metrics_short_qna function is already defined and can be reused.\n",
        "# NLTK punkt_tab should also be downloaded (done in Cell 13 config for long Q&A).\n",
        "\n",
        "print(\"\\nSetting up Trainer for LONG Q&A task...\")\n",
        "trainer_long = Seq2SeqTrainer(\n",
        "    model=model_long,\n",
        "    args=args_long,\n",
        "    train_dataset=tokenized_datasets_long[\"train\"],\n",
        "    eval_dataset=tokenized_datasets_long[\"validation\"],\n",
        "    data_collator=data_collator_long,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics_short_qna # Reusing the metrics function\n",
        ")\n",
        "print(\"Trainer for LONG Q&A setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "60SJH4u3fAvq",
        "outputId": "fe88fdd0-e0dc-46bc-c8a8-269962ff0db5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training for LONG Q&A Generation...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1812' max='1812' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1812/1812 17:08, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.333300</td>\n",
              "      <td>2.034157</td>\n",
              "      <td>55.604900</td>\n",
              "      <td>37.607300</td>\n",
              "      <td>50.229100</td>\n",
              "      <td>54.111000</td>\n",
              "      <td>70.057100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.152200</td>\n",
              "      <td>2.139813</td>\n",
              "      <td>55.994900</td>\n",
              "      <td>37.613700</td>\n",
              "      <td>51.045100</td>\n",
              "      <td>54.574400</td>\n",
              "      <td>68.102100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.124100</td>\n",
              "      <td>2.197016</td>\n",
              "      <td>55.469600</td>\n",
              "      <td>37.577800</td>\n",
              "      <td>50.885300</td>\n",
              "      <td>54.270900</td>\n",
              "      <td>68.733600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.111500</td>\n",
              "      <td>2.230448</td>\n",
              "      <td>55.354600</td>\n",
              "      <td>37.703500</td>\n",
              "      <td>51.154900</td>\n",
              "      <td>54.130500</td>\n",
              "      <td>69.662600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.104500</td>\n",
              "      <td>2.258020</td>\n",
              "      <td>55.170100</td>\n",
              "      <td>37.665300</td>\n",
              "      <td>50.968600</td>\n",
              "      <td>54.060300</td>\n",
              "      <td>70.211100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.101800</td>\n",
              "      <td>2.263139</td>\n",
              "      <td>55.002700</td>\n",
              "      <td>37.611900</td>\n",
              "      <td>50.899300</td>\n",
              "      <td>53.837500</td>\n",
              "      <td>69.524200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LONG Q&A training finished.\n",
            "\n",
            "Training for LONG Q&A finished. Saving final model and metrics...\n",
            "Final Long Q&A model (potentially best) saved to /content/drive/MyDrive/Testing/New/model_outputs/long_qna_finetuned\n",
            "***** train_long metrics *****\n",
            "  epoch                    =        6.0\n",
            "  total_flos               =  2673203GF\n",
            "  train_loss               =     0.2696\n",
            "  train_runtime            = 0:17:09.03\n",
            "  train_samples_per_second =     14.069\n",
            "  train_steps_per_second   =      1.761\n",
            "Best LONG Q&A Model explicitly saved to /content/drive/MyDrive/Testing/New/model_outputs/long_qna_finetuned/best_model\n",
            "Cleaning up LONG Q&A model and trainer from GPU memory...\n",
            "Training process complete for LONG Q&A.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStarting training for LONG Q&A Generation...\")\n",
        "try:\n",
        "    train_result_long = trainer_long.train()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during LONG Q&A training: {e}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Attempting to clear CUDA cache for LONG Q&A training...\")\n",
        "        torch.cuda.empty_cache()\n",
        "    raise e\n",
        "print(\"LONG Q&A training finished.\")\n",
        "\n",
        "print(\"\\nTraining for LONG Q&A finished. Saving final model and metrics...\")\n",
        "\n",
        "trainer_long.save_model() # Saves to args_long.output_dir (long_qna_output_dir)\n",
        "print(f\"Final Long Q&A model (potentially best) saved to {long_qna_output_dir}\")\n",
        "\n",
        "metrics_long = train_result_long.metrics\n",
        "trainer_long.log_metrics(\"train_long\", metrics_long)\n",
        "trainer_long.save_metrics(\"train_long\", metrics_long)\n",
        "trainer_long.save_state()\n",
        "\n",
        "# === PATH CORRECTION FOR SAVING BEST LONG MODEL ===\n",
        "long_qna_best_model_path = os.path.join(long_qna_output_dir, \"best_model\")\n",
        "if not os.path.exists(long_qna_best_model_path):\n",
        "    os.makedirs(long_qna_best_model_path)\n",
        "trainer_long.save_model(long_qna_best_model_path)\n",
        "print(f\"Best LONG Q&A Model explicitly saved to {long_qna_best_model_path}\")\n",
        "# === PATH CORRECTION END ===\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Cleaning up LONG Q&A model and trainer from GPU memory...\")\n",
        "    if 'model_long' in globals(): del model_long\n",
        "    if 'trainer_long' in globals(): del trainer_long\n",
        "    torch.cuda.empty_cache()\n",
        "print(\"Training process complete for LONG Q&A.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3OSKv4_fNOC",
        "outputId": "21e70703-ab8b-4c3a-bd54-ced385da99ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading fine-tuned LONG Q&A model and tokenizer from: /content/drive/MyDrive/Testing/New/model_outputs/long_qna_finetuned/best_model\n",
            "LONG Q&A Inference model moved to device: cpu\n",
            "\n",
            "--- Example LONG Q&A Inference ---\n",
            "Input Passage for LONG Q&A:\n",
            "\n",
            "Intrusion detection plays a crucial role in network security, as it helps ensure proper handling and protection of resources. Packet filtering plays a crucial role in network security, as it helps ensure proper handling and protection of resources. ...\n",
            "\n",
            "Generating LONG question and answer...\n",
            "Raw generated LONG output: question: What is packet filtering in network security? answer: Firewall configuration refers to the process used to packet filtrating in system security. It involves multiple steps to ensure accuracy and reliability. In network safety, proper execution of packet filters is essential for maintaining system integrity. Failures in this process can lead to significant security vulnerabilities.\n",
            "Parsing successful for LONG Q&A.\n",
            "\n",
            "Generated LONG Q&A Pair:\n",
            "  Q: What is packet filtering in network security?\n",
            "  A: Firewall configuration refers to the process used to packet filtrating in system security. It involves multiple steps to ensure accuracy and reliability. In network safety, proper execution of packet filters is essential for maintaining system integrity. Failures in this process can lead to significant security vulnerabilities.\n",
            "\n",
            "--- LONG Q&A Inference Cell Complete ---\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import re\n",
        "import os\n",
        "\n",
        "long_qna_model_load_path_inf = os.path.join(long_qna_output_dir, \"best_model\")\n",
        "\n",
        "model_checkpoint_long_inf = \"t5-small\"\n",
        "max_input_length_long_inf = 750\n",
        "input_prefix_long_inf = \"generate question and answer: context: \"\n",
        "inf_max_output_length_long = 256\n",
        "inf_num_beams_long = 4\n",
        "inf_early_stopping_long = True\n",
        "inf_no_repeat_ngram_size_long = 2\n",
        "\n",
        "\n",
        "if not os.path.exists(long_qna_model_load_path_inf):\n",
        "    raise FileNotFoundError(f\"Fine-tuned LONG Q&A model directory not found: {long_qna_model_load_path_inf}.\")\n",
        "\n",
        "print(f\"Loading fine-tuned LONG Q&A model and tokenizer from: {long_qna_model_load_path_inf}\")\n",
        "tokenizer_inf_long = AutoTokenizer.from_pretrained(long_qna_model_load_path_inf)\n",
        "model_inf_long = AutoModelForSeq2SeqLM.from_pretrained(long_qna_model_load_path_inf)\n",
        "\n",
        "device_inf_long = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_inf_long.to(device_inf_long)\n",
        "model_inf_long.eval()\n",
        "print(f\"LONG Q&A Inference model moved to device: {device_inf_long}\")\n",
        "\n",
        "def generate_long_qna(context):\n",
        "    if not context or not isinstance(context, str):\n",
        "        print(\"Error: Invalid context provided.\")\n",
        "        return None\n",
        "    input_text = f\"{input_prefix_long_inf}{context.strip()}\"\n",
        "    inputs = tokenizer_inf_long(input_text,\n",
        "                                max_length=max_input_length_long_inf,\n",
        "                                padding=True,\n",
        "                                truncation=True,\n",
        "                                return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(device_inf_long)\n",
        "    attention_mask = inputs.attention_mask.to(device_inf_long)\n",
        "\n",
        "    print(f\"\\nGenerating LONG question and answer...\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model_inf_long.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=inf_max_output_length_long,\n",
        "            num_beams=inf_num_beams_long,\n",
        "            early_stopping=inf_early_stopping_long,\n",
        "            no_repeat_ngram_size=inf_no_repeat_ngram_size_long,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "    generated_text = tokenizer_inf_long.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    print(f\"Raw generated LONG output: {generated_text}\")\n",
        "\n",
        "    match = re.match(r\"question:\\s*(.*?)\\s*answer:\\s*(.*)\", generated_text, re.IGNORECASE | re.DOTALL)\n",
        "    if match:\n",
        "        question = match.group(1).strip()\n",
        "        answer = match.group(2).strip()\n",
        "        print(\"Parsing successful for LONG Q&A.\")\n",
        "        return {\"question\": question, \"answer\": answer}\n",
        "    else:\n",
        "        print(\"Error: Could not parse the generated LONG output.\")\n",
        "        parts = generated_text.lower().split('answer:', 1)\n",
        "        if len(parts) == 2:\n",
        "             q_part = parts[0].replace('question:', '').strip()\n",
        "             a_part = parts[1].strip()\n",
        "             if q_part and a_part:\n",
        "                 print(\"Fallback parsing attempted for LONG Q&A.\")\n",
        "                 return {\"question\": q_part, \"answer\": a_part}\n",
        "        return None\n",
        "\n",
        "passage_example_long = \"\"\"\n",
        "Intrusion detection plays a crucial role in network security, as it helps ensure proper handling and protection of resources. Packet filtering plays a crucial role in network security, as it helps ensure proper handling and protection of resources. Vpn plays a crucial role in network security, as it helps ensure proper handling and protection of resources. Network segmentation plays a crucial role in network security, as it helps ensure proper handling and protection of resources. Network segmentation plays a crucial role in network security, as it helps ensure proper handling and protection of resources. Vpn plays a crucial role in network security, as it helps ensure proper handling and protection of resources. Network segmentation plays a crucial role in network security, as it helps ensure proper handling and protection of resources. Firewall configuration plays a crucial role in network security, as it helps ensure proper handling and protection of resources. Network segmentation plays a crucial role in network security, as it helps ensure proper handling and protection of resources. Vpn plays a crucial role in network security, as it helps ensure proper handling and protection of resources. Intrusion detection plays a crucial role in network security, as it helps ensure proper handling and protection of resources. Network segmentation plays a crucial role in network security, as it helps ensure proper handling and protection of resources.\n",
        "\"\"\"\n",
        "print(\"\\n--- Example LONG Q&A Inference ---\")\n",
        "print(f\"Input Passage for LONG Q&A:\\n{passage_example_long[:250]}...\")\n",
        "long_qna_pair = generate_long_qna(passage_example_long)\n",
        "if long_qna_pair:\n",
        "    print(\"\\nGenerated LONG Q&A Pair:\")\n",
        "    print(f\"  Q: {long_qna_pair['question']}\")\n",
        "    print(f\"  A: {long_qna_pair['answer']}\")\n",
        "else:\n",
        "    print(\"\\nFailed to generate a valid LONG Q&A pair.\")\n",
        "print(\"\\n--- LONG Q&A Inference Cell Complete ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlRnfdDQxcc0",
        "outputId": "b2243a6b-ccc1-4a5c-ab33-b8a64aef8512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (3.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.4.26)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        }
      ],
      "source": [
        "!pip install Flask PyPDF2\n",
        "!pip install flask-ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZSHsbKS1XCa",
        "outputId": "a43933ce-cda1-45ee-ee7a-16ad2def31d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.9-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.9\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mXn7_Hx2a2O",
        "outputId": "462b0927-7d31-4b83-99dd-dd3373ce1b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2xkc0M26NfumNL9ASRxPB1ji1Ak_4J9UXseie1BgEDBUCMUc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AehFmjEWfOdE",
        "outputId": "527d9f16-ecaa-41d0-c9ca-3f0813562f08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created/Updated templates/index.html\n",
            "Flask App: Loading models...\n",
            "Flask App: Using device: cpu\n",
            "Flask App: Loading Short Q&A model from: /content/drive/MyDrive/Testing/New/model_outputs/short_qna_finetuned/best_model\n",
            "Flask App: Short Q&A model loaded.\n",
            "Flask App: Loading Long Q&A model from: /content/drive/MyDrive/Testing/New/model_outputs/long_qna_finetuned/best_model\n",
            "Flask App: Long Q&A model loaded.\n",
            "Flask App: Model loading attempt complete.\n",
            " * ngrok tunnel \"NgrokTunnel: \"https://7ae5-34-125-30-224.ngrok-free.app\" -> \"http://localhost:5000\"\" -> \"http://127.0.0.1:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [29/May/2025 08:11:25] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/May/2025 08:11:26] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App: Generating 3 short Q&A pairs with temp=0.9, top_p=0.9, top_k=0, beams=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/May/2025 08:11:55] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App: Raw generated output #1 (short): question: What is object-oriented programming? answer: Object-oriented programs organize code into classes that encapsulate data and behavior as objects.\n",
            "Flask App: Raw generated output #2 (short): question: Why is static typing important? answer: Static typing enforces type rules during compilation, whereas static typing checks types during execution.\n",
            "Flask App: Raw generated output #3 (short): question: What is dynamic typing? answer: Static typing enforces type rules during compilation, whereas dynamic typing checks types during execution.\n",
            "Flask App: Generating 3 short Q&A pairs with temp=0.9, top_p=0.9, top_k=0, beams=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/May/2025 08:13:36] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App: Raw generated output #1 (short): question: Why is dynamic typing important? answer: Static typing enforces type rules during compilation and execution, while dynamic typing checks types during execution.\n",
            "Flask App: Raw generated output #2 (short): question: Why is static typing important? answer: Static typing enforces type rules during compilation, whereas static typing enforced type rules in machine code during execution.\n",
            "Flask App: Raw generated output #3 (short): question: Why is dynamic typing important? answer: Static typing enforces type rules during compilation, whereas dynamic typing checks types during execution.\n",
            "Flask App: De-duplicated Q&A pairs. Original: 3, Unique: 2\n",
            "Flask App: Generating 3 short Q&A pairs with temp=0.9, top_p=0.9, top_k=0, beams=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/May/2025 08:18:05] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App: Raw generated output #1 (short): question: What is security monitoring in cloud security? answer: Security monitoring refers to the process used to security monitoring and monitoring in Cloud security.\n",
            "Flask App: Raw generated output #2 (short): question: What is data encryption in cloud security? answer: Data encryption refers to the process used to data encryption during auditing in cloud cybersecurity.\n",
            "Flask App: Raw generated output #3 (short): question: What is critical monitoring in cloud security? answer: Critical monitoring refers to the process used to critical monitoring during security auditing in cloud privacy.\n",
            "Flask App: Generating 3 short Q&A pairs with temp=0.9, top_p=0.9, top_k=0, beams=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/May/2025 08:20:00] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App: Raw generated output #1 (short): question: What is chain of custody in digital forensics? answer: Chain of custody refers to the process used to chain of garde in digital documentation.\n",
            "Flask App: Raw generated output #2 (short): question: What is chain of custody in digital forensics? answer: Chain of custody refers to the process used to chain of garde in digital contradictions.\n",
            "Flask App: Raw generated output #3 (short): question: What is chain of custody in digital forensics? answer: Chain of custody refers to the process used to chain of garde in digital thematics.\n",
            "Flask App: De-duplicated Q&A pairs. Original: 3, Unique: 1\n",
            "Flask App: Generating 3 short Q&A pairs with temp=0.9, top_p=0.9, top_k=0, beams=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/May/2025 08:20:45] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App: Raw generated output #1 (short): question: What is chain of custody in digital forensics? answer: Chain of custody refers to the process used to chain of garde in digital digital judicial system.\n",
            "Flask App: Raw generated output #2 (short): question: What is chain of custody in digital forensics? answer: Chain of custody refers to the process used to chain of court in digital neutrals.\n",
            "Flask App: Raw generated output #3 (short): question: What is chain of custody in digital forensics? answer: Chain of custody refers to the process used to chain of possession in digital contradictions.\n",
            "Flask App: De-duplicated Q&A pairs. Original: 3, Unique: 1\n",
            "Flask App: Generating 3 long Q&A pairs with temp=0.9, top_p=0.9, top_k=0, beams=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/May/2025 08:21:02] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App: Raw generated output #1 (long): question: What is chain of custody in digital forensics? answer: Chain of custody refers to the process used to chain of garde in digital digital criterion. It involves multiple steps to ensure accuracy and reliability. In digital cryptics, proper execution of chain of court is essential for maintaining system integrity. Failures in this process can lead to significant security vulnerabilities.\n",
            "Flask App: Raw generated output #2 (long): question: What is chain of custody in digital forensics? answer: Chain of custody refers to the process used to chain of garde in digital digital. forensic culture: It involves multiple steps to ensure accuracy and reliability. In digital, proper execution of chain of hold is essential for maintaining system integrity. Failures in this process can lead to significant security vulnerabilities.\n",
            "Flask App: Raw generated output #3 (long): question: What is chain of custody in digital forensics? answer: Chain of custody refers to the process used to chain of garde in digital digital prosecutors. It involves multiple steps to ensure accuracy and reliability. In digital judicial processes, proper execution of chain of hold is essential for maintaining system integrity. Failures in this process can lead to significant security vulnerabilities.\n",
            "Flask App: De-duplicated Q&A pairs. Original: 3, Unique: 1\n",
            "Flask App: Generating 3 long Q&A pairs with temp=0.9, top_p=0.9, top_k=0, beams=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/May/2025 08:23:27] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App: Raw generated output #1 (long): question: What is compliance in cloud security? answer: Compliance refers to the process used to compliance in Cloud Security. It involves multiple steps to ensure accuracy and reliability. In cloud security, proper execution of compliance is essential for maintaining system integrity. Failures in this process can lead to significant security vulnerabilities.\n",
            "Flask App: Raw generated output #2 (long): question: What is compliance in cloud security? answer: Compliance refers to the process used to compliance in Cloud security. It involves multiple steps to ensure accuracy and reliability. In cloud security, proper execution of compliance is essential for maintaining system integrity. Failures in this process can lead to significant security vulnerabilities.\n",
            "Flask App: Raw generated output #3 (long): question: What is security policy in cloud security? answer: Security policy refers to the process used to security policy for cloud security. It involves multiple steps to ensure accuracy and reliability. In cloud security, proper execution of security policy is essential for maintaining system integrity. Failures in this process can lead to significant security vulnerabilities.\n",
            "Flask App: De-duplicated Q&A pairs. Original: 3, Unique: 2\n",
            "Flask App: Generating 3 long Q&A pairs with temp=0.9, top_p=0.9, top_k=0, beams=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/May/2025 08:24:29] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App: Raw generated output #1 (long): question: What is compliance in cloud security? answer: Compliance refers to the process used to compliance in Cloud security. It involves multiple steps to ensure accuracy and reliability. In cloud security, proper execution of compliance is essential for maintaining system integrity. Failures in this process can lead to significant security vulnerabilities.\n",
            "Flask App: Raw generated output #2 (long): question: What is security policy in cloud security? answer: Security policy refers to the process used to security policy în cloud security. It involves multiple steps to ensure accuracy and reliability. In cloud security, proper execution of security policy is essential for maintaining system integrity. Failures in this process can lead to significant security vulnerabilities.\n",
            "Flask App: Raw generated output #3 (long): question: What is security policy in cloud security? answer: Security policy refers to the process used to security policy for security management. It involves multiple steps to ensure accuracy and reliability. In cloud security, proper execution of security policy is essential for maintaining system integrity. Failures in this process can lead to significant security vulnerabilities.\n",
            "Flask App: De-duplicated Q&A pairs. Original: 3, Unique: 2\n",
            "Flask App: Generating 3 long Q&A pairs with temp=0.9, top_p=0.9, top_k=0, beams=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/May/2025 08:24:45] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App: Raw generated output #1 (long): question: What is security policy in cloud security? answer: Security policy refers to the process used to security policy în cloud security. It involves multiple steps to ensure accuracy and reliability. In cloud security, proper execution of security policy is essential for maintaining system integrity. Failures in this process can lead to significant security vulnerabilities.\n",
            "Flask App: Raw generated output #2 (long): question: What is security policy in cloud security? answer: Security policy refers to the process used to security policy for security management. It involves multiple steps to ensure accuracy and reliability. In cloud security, proper execution of security policy is essential for maintaining system integrity. Failures in this process can lead to significant security vulnerabilities.\n",
            "Flask App: Raw generated output #3 (long): question: What is compliance in cloud security? answer: Compliance refers to the process used to compliance in Cloud security. It involves multiple steps to ensure accuracy and reliability. In cloud security, proper execution of compliance is essential for maintaining system integrity. Failures in this process can lead to significant security vulnerabilities.\n",
            "Flask App: De-duplicated Q&A pairs. Original: 3, Unique: 2\n",
            "Flask App: Generating 3 short Q&A pairs with temp=0.9, top_p=0.9, top_k=0, beams=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/May/2025 08:24:56] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App: Raw generated output #1 (short): question: What is identity and access management in cloud security? answer: Identity and access control refers to the process used to identity and credit management in cyber security.\n",
            "Flask App: Raw generated output #2 (short): question: What is key management in cloud security? answer: Key management refers to the process used to key management for cloud security.\n",
            "Flask App: Raw generated output #3 (short): question: What is identity and access management in cloud security? answer: Identity and access control refers to the process used to identity and Access management in Cloud security.\n",
            "Flask App: De-duplicated Q&A pairs. Original: 3, Unique: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/May/2025 08:26:00] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App: Generating 3 short Q&A pairs with temp=0.9, top_p=0.9, top_k=0, beams=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/May/2025 09:36:24] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask App: Raw generated output #1 (short): question: What is data encryption in cloud security? answer: Data encryption refers to the process used to data encryption for data encryption.\n",
            "Flask App: Raw generated output #2 (short): question: What is identity and access management in cloud security? answer: Identity and access control refers to the process used to identity and identity management in Cloud security.\n",
            "Flask App: Raw generated output #3 (short): question: What is compliance in cloud security? answer: Compliance refers to the process used to compliance in clouds security.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from flask import Flask, render_template, request\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import PyPDF2\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "try:\n",
        "    _short_qna_finetuned_dir_flask = short_qna_output_dir\n",
        "    _long_qna_finetuned_dir_flask = long_qna_output_dir\n",
        "except NameError:\n",
        "    print(\"Flask: Notebook output_dir variables not found, using fallback paths.\")\n",
        "    _base_drive_output_path_flask = \"/content/drive/MyDrive/Testing/New/model_outputs\"\n",
        "    _short_qna_finetuned_dir_flask = os.path.join(_base_drive_output_path_flask, \"short_qna_finetuned\")\n",
        "    _long_qna_finetuned_dir_flask = os.path.join(_base_drive_output_path_flask, \"long_qna_finetuned\")\n",
        "\n",
        "SHORT_QNA_MODEL_LOAD_PATH = os.path.join(_short_qna_finetuned_dir_flask, \"best_model\")\n",
        "LONG_QNA_MODEL_LOAD_PATH = os.path.join(_long_qna_finetuned_dir_flask, \"best_model\")\n",
        "\n",
        "MODEL_CHECKPOINT_INF = \"t5-small\"\n",
        "\n",
        "# === MORE AGGRESSIVE DIVERSITY PARAMETERS ===\n",
        "# Short Q&A Inference Params\n",
        "MAX_INPUT_LENGTH_SHORT_INF = 750\n",
        "INPUT_PREFIX_SHORT_INF = \"generate question and answer: context: \"\n",
        "INF_MAX_OUTPUT_LENGTH_SHORT = 128\n",
        "INF_NUM_BEAMS_SHORT = 1                 # <<< SET TO 1 FOR PURE SAMPLING\n",
        "INF_EARLY_STOPPING_SHORT = False        # <<< Disable early stopping with num_beams=1 if using do_sample=True\n",
        "                                        # (or keep True, but it has less effect with num_beams=1)\n",
        "INF_NO_REPEAT_NGRAM_SIZE_SHORT = 3\n",
        "NUM_RETURN_SEQUENCES_SHORT = 3          # Let's try to get 3\n",
        "TEMPERATURE_SHORT = 0.9                 # <<< INCREASED TEMPERATURE\n",
        "TOP_P_SHORT = 0.9                       # Keep top_p\n",
        "TOP_K_SHORT = 0                         # <<< SET top_k=0 to disable it and rely on top_p & temp\n",
        "\n",
        "# Long Q&A Inference Params\n",
        "MAX_INPUT_LENGTH_LONG_INF = 750\n",
        "INPUT_PREFIX_LONG_INF = \"generate question and answer: context: \"\n",
        "INF_MAX_OUTPUT_LENGTH_LONG = 256\n",
        "INF_NUM_BEAMS_LONG = 1                  # <<< SET TO 1 FOR PURE SAMPLING\n",
        "INF_EARLY_STOPPING_LONG = False         # <<< Disable\n",
        "INF_NO_REPEAT_NGRAM_SIZE_LONG = 3\n",
        "NUM_RETURN_SEQUENCES_LONG = 3           # Let's try to get 3 for long as well\n",
        "TEMPERATURE_LONG = 0.9                  # <<< INCREASED TEMPERATURE\n",
        "TOP_P_LONG = 0.9                        # Keep top_p\n",
        "TOP_K_LONG = 0                          # <<< SET top_k=0 to disable it\n",
        "# === END OF DIVERSITY PARAMETER CHANGES ===\n",
        "\n",
        "tokenizer_short_flask = None\n",
        "model_short_flask = None\n",
        "tokenizer_long_flask = None\n",
        "model_long_flask = None\n",
        "device_flask = None\n",
        "\n",
        "def load_models_for_flask():\n",
        "    global tokenizer_short_flask, model_short_flask, tokenizer_long_flask, model_long_flask, device_flask\n",
        "    print(\"Flask App: Loading models...\")\n",
        "    device_flask = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Flask App: Using device: {device_flask}\")\n",
        "    if os.path.exists(SHORT_QNA_MODEL_LOAD_PATH):\n",
        "        print(f\"Flask App: Loading Short Q&A model from: {SHORT_QNA_MODEL_LOAD_PATH}\")\n",
        "        try:\n",
        "            tokenizer_short_flask = AutoTokenizer.from_pretrained(SHORT_QNA_MODEL_LOAD_PATH)\n",
        "            model_short_flask = AutoModelForSeq2SeqLM.from_pretrained(SHORT_QNA_MODEL_LOAD_PATH)\n",
        "            model_short_flask.to(device_flask)\n",
        "            model_short_flask.eval()\n",
        "            print(\"Flask App: Short Q&A model loaded.\")\n",
        "        except Exception as e: print(f\"Flask App: Error loading Short Q&A model: {e}\"); model_short_flask = None\n",
        "    else: print(f\"Flask App Warning: Short Q&A model path not found: {SHORT_QNA_MODEL_LOAD_PATH}\")\n",
        "    if os.path.exists(LONG_QNA_MODEL_LOAD_PATH):\n",
        "        print(f\"Flask App: Loading Long Q&A model from: {LONG_QNA_MODEL_LOAD_PATH}\")\n",
        "        try:\n",
        "            tokenizer_long_flask = AutoTokenizer.from_pretrained(LONG_QNA_MODEL_LOAD_PATH)\n",
        "            model_long_flask = AutoModelForSeq2SeqLM.from_pretrained(LONG_QNA_MODEL_LOAD_PATH)\n",
        "            model_long_flask.to(device_flask)\n",
        "            model_long_flask.eval()\n",
        "            print(\"Flask App: Long Q&A model loaded.\")\n",
        "        except Exception as e: print(f\"Flask App: Error loading Long Q&A model: {e}\"); model_long_flask = None\n",
        "    else: print(f\"Flask App Warning: Long Q&A model path not found: {LONG_QNA_MODEL_LOAD_PATH}\")\n",
        "    print(\"Flask App: Model loading attempt complete.\")\n",
        "\n",
        "def parse_generated_qna_text(generated_text):\n",
        "    match = re.match(r\"question:\\s*(.*?)\\s*answer:\\s*(.*)\", generated_text, re.IGNORECASE | re.DOTALL)\n",
        "    if match:\n",
        "        question, answer = match.group(1).strip(), match.group(2).strip()\n",
        "        if question and answer: return {\"question\": question, \"answer\": answer}\n",
        "    parts = generated_text.lower().split('answer:', 1)\n",
        "    if len(parts) == 2:\n",
        "        q_part, a_part = parts[0].replace('question:', '').strip(), parts[1].strip()\n",
        "        if q_part and a_part: print(\"Fallback parsing used.\"); return {\"question\": q_part, \"answer\": a_part}\n",
        "    return None\n",
        "\n",
        "def generate_multiple_qna_from_model(context, answer_type=\"short\"):\n",
        "    qna_list, error_message = [], None\n",
        "    if answer_type == \"short\":\n",
        "        if not model_short_flask or not tokenizer_short_flask: return [], \"Short Q&A model not loaded.\"\n",
        "        tokenizer_inf, model_inf = tokenizer_short_flask, model_short_flask\n",
        "        params = {\"prefix\": INPUT_PREFIX_SHORT_INF, \"max_in\": MAX_INPUT_LENGTH_SHORT_INF, \"max_out\": INF_MAX_OUTPUT_LENGTH_SHORT,\n",
        "                  \"beams\": INF_NUM_BEAMS_SHORT, \"early_stop\": INF_EARLY_STOPPING_SHORT, \"no_repeat\": INF_NO_REPEAT_NGRAM_SIZE_SHORT,\n",
        "                  \"num_seq\": NUM_RETURN_SEQUENCES_SHORT, \"temp\": TEMPERATURE_SHORT, \"top_p\": TOP_P_SHORT, \"top_k\": TOP_K_SHORT}\n",
        "    elif answer_type == \"long\":\n",
        "        if not model_long_flask or not tokenizer_long_flask: return [], \"Long Q&A model not loaded.\"\n",
        "        tokenizer_inf, model_inf = tokenizer_long_flask, model_long_flask\n",
        "        params = {\"prefix\": INPUT_PREFIX_LONG_INF, \"max_in\": MAX_INPUT_LENGTH_LONG_INF, \"max_out\": INF_MAX_OUTPUT_LENGTH_LONG,\n",
        "                  \"beams\": INF_NUM_BEAMS_LONG, \"early_stop\": INF_EARLY_STOPPING_LONG, \"no_repeat\": INF_NO_REPEAT_NGRAM_SIZE_LONG,\n",
        "                  \"num_seq\": NUM_RETURN_SEQUENCES_LONG, \"temp\": TEMPERATURE_LONG, \"top_p\": TOP_P_LONG, \"top_k\": TOP_K_LONG}\n",
        "    else: return [], \"Invalid answer type specified.\"\n",
        "\n",
        "    if not context or not isinstance(context, str): return [], \"Invalid context provided.\"\n",
        "    input_text = f\"{params['prefix']}{context.strip()}\"\n",
        "    try:\n",
        "        inputs = tokenizer_inf(input_text, max_length=params['max_in'], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "    except Exception as e: return [], f\"Error during tokenization: {str(e)}\"\n",
        "\n",
        "    input_ids, attention_mask = inputs.input_ids.to(device_flask), inputs.attention_mask.to(device_flask)\n",
        "    print(f\"Flask App: Generating {params['num_seq']} {answer_type} Q&A pairs with temp={params['temp']}, top_p={params['top_p']}, top_k={params['top_k']}, beams={params['beams']}...\")\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            generation_args = {\n",
        "                \"input_ids\": input_ids, \"attention_mask\": attention_mask, \"max_length\": params['max_out'],\n",
        "                \"num_return_sequences\": params['num_seq'], \"no_repeat_ngram_size\": params['no_repeat'],\n",
        "                \"do_sample\": True, \"temperature\": params['temp'], \"top_p\": params['top_p'], \"top_k\": params['top_k']\n",
        "            }\n",
        "            # Only include num_beams and early_stopping if num_beams > 1\n",
        "            if params['beams'] > 1:\n",
        "                generation_args[\"num_beams\"] = params['beams']\n",
        "                generation_args[\"early_stopping\"] = params['early_stop']\n",
        "\n",
        "            outputs = model_inf.generate(**generation_args)\n",
        "    except Exception as e: return [], f\"Error during model generation: {str(e)}\"\n",
        "\n",
        "    for i, output_sequence in enumerate(outputs):\n",
        "        generated_text = tokenizer_inf.decode(output_sequence, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "        print(f\"Flask App: Raw generated output #{i+1} ({answer_type}): {generated_text}\")\n",
        "        parsed_qna = parse_generated_qna_text(generated_text)\n",
        "        if parsed_qna: qna_list.append(parsed_qna)\n",
        "    if not qna_list and params['num_seq'] > 0: error_message = f\"Could not parse any valid Q&A pairs from the generated {answer_type} output.\"\n",
        "\n",
        "    # Simple de-duplication based on question\n",
        "    if qna_list:\n",
        "        unique_qna_list = []\n",
        "        seen_questions = set()\n",
        "        for qna_pair in qna_list:\n",
        "            question_key = qna_pair[\"question\"].lower().strip()\n",
        "            if question_key not in seen_questions:\n",
        "                unique_qna_list.append(qna_pair)\n",
        "                seen_questions.add(question_key)\n",
        "        if len(unique_qna_list) < len(qna_list):\n",
        "            print(f\"Flask App: De-duplicated Q&A pairs. Original: {len(qna_list)}, Unique: {len(unique_qna_list)}\")\n",
        "        qna_list = unique_qna_list\n",
        "\n",
        "    return qna_list, error_message\n",
        "\n",
        "def extract_text_from_txt(file_stream):\n",
        "    try: return file_stream.read().decode('utf-8')\n",
        "    except Exception as e: print(f\"Error reading txt: {e}\"); return None\n",
        "\n",
        "def extract_text_from_pdf(file_stream):\n",
        "    try:\n",
        "        reader = PyPDF2.PdfReader(file_stream)\n",
        "        text = \"\".join(page.extract_text() or \"\" for page in reader.pages if page.extract_text())\n",
        "        return text.strip() if text else \"\"\n",
        "    except Exception as e: print(f\"Error reading PDF: {e}\"); return None\n",
        "\n",
        "WORD_LIMIT = 300\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def index():\n",
        "    error, passage_input, answer_type_input, qna_results = None, \"\", \"short\", []\n",
        "    if request.method == 'POST':\n",
        "        passage_text_form, uploaded_file = request.form.get('passage_text', '').strip(), request.files.get('file')\n",
        "        answer_type_input = request.form.get('answer_type', 'short')\n",
        "        context_to_process = \"\"\n",
        "        if uploaded_file and uploaded_file.filename != '':\n",
        "            filename = uploaded_file.filename\n",
        "            if filename.endswith('.txt'): context_to_process = extract_text_from_txt(uploaded_file.stream)\n",
        "            elif filename.endswith('.pdf'): context_to_process = extract_text_from_pdf(uploaded_file.stream)\n",
        "            else: error = \"Invalid file type. Upload .txt or .pdf.\"\n",
        "            if context_to_process is None and not error: error = \"Could not extract text from file.\"\n",
        "            elif context_to_process == \"\" and not error : error = \"Extracted text from file is empty.\" # Check for empty string explicitly\n",
        "            passage_input = context_to_process[:1000] + \"...\" if context_to_process and len(context_to_process) > 1000 else (context_to_process or f\"File: {filename} (failed/empty)\")\n",
        "        elif passage_text_form: context_to_process = passage_input = passage_text_form\n",
        "        else: error = \"Please enter a passage or upload a file.\"\n",
        "\n",
        "        if context_to_process and not error:\n",
        "            word_count = len(context_to_process.split())\n",
        "            if word_count == 0 : error = \"The provided text is empty after processing.\" if not error else error\n",
        "            elif word_count > WORD_LIMIT: error = f\"Passage exceeds word limit of {WORD_LIMIT} (found {word_count}).\"\n",
        "            else:\n",
        "                model_ready = (answer_type_input == \"short\" and model_short_flask) or \\\n",
        "                              (answer_type_input == \"long\" and model_long_flask)\n",
        "                if not model_ready: error = f\"The model for '{answer_type_input}' answers is not loaded.\"\n",
        "                else:\n",
        "                    generated_qna_list, gen_error = generate_multiple_qna_from_model(context_to_process, answer_type_input)\n",
        "                    if gen_error: error = gen_error\n",
        "                    elif generated_qna_list: qna_results = generated_qna_list\n",
        "                    else: error = f\"No valid Q&A pairs generated for '{answer_type_input}' type.\"\n",
        "        elif not passage_text_form and not (uploaded_file and uploaded_file.filename != '') and not error: # User clicked submit with no input\n",
        "            error = \"Please enter a passage or upload a file.\"\n",
        "        elif not context_to_process and passage_text_form and not error : # Text area had only whitespace\n",
        "             error = \"Please enter a non-empty passage.\"\n",
        "             passage_input = passage_text_form\n",
        "\n",
        "    return render_template('index.html', error=error, passage_input=passage_input,\n",
        "                           answer_type_input=answer_type_input, qna_results=qna_results, WORD_LIMIT=WORD_LIMIT)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if not os.path.exists(\"templates\"): os.makedirs(\"templates\")\n",
        "    html_content = \"\"\"\n",
        "<!doctype html><html lang=\"en\"><head><meta charset=\"utf-8\"><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"><title>Q&A Generator</title><style>body{font-family:sans-serif;margin:20px;background-color:#f4f4f4;color:#333}.container{background-color:#fff;padding:20px;border-radius:8px;box-shadow:0 0 10px rgba(0,0,0,0.1);max-width:800px;margin:auto}h1,h2{color:#333;text-align:center}label{display:block;margin-top:15px;margin-bottom:5px;font-weight:bold}textarea{width:100%;padding:10px;border:1px solid #ddd;border-radius:4px;box-sizing:border-box;min-height:150px}select,input[type=\"file\"]{width:100%;padding:10px;margin-top:5px;border:1px solid #ddd;border-radius:4px;box-sizing:border-box}input[type=\"submit\"]{background-color:#5cb85c;color:white;cursor:pointer;font-size:16px;padding:12px 20px;border:none;margin-top:20px;width:100%}input[type=\"submit\"]:hover{background-color:#4cae4c}.error{color:#d9534f;background-color:#f2dede;border:1px solid #ebccd1;padding:10px;border-radius:4px;margin-top:15px}.qna-results-container{margin-top:30px}.qna-pair{border:1px solid #eee;padding:15px;margin-top:15px;border-radius:4px;background-color:#f9f9f9}.qna-pair p{margin:8px 0;line-height:1.6}.qna-pair strong{color:#0056b3}</style></head><body><div class=\"container\"><h1>Q&A Generator</h1>{% if error %}<p class=\"error\">{{ error }}</p>{% endif %}<form method=\"POST\" enctype=\"multipart/form-data\"><div><label for=\"passage_text\">Enter Passage (max {{ WORD_LIMIT }} words):</label><textarea name=\"passage_text\" id=\"passage_text\" rows=\"10\">{{ passage_input if passage_input is not none else '' }}</textarea></div><div><label for=\"file\">Or Upload File (.txt, .pdf):</label><input type=\"file\" name=\"file\" id=\"file\" accept=\".txt,.pdf\"></div><div><label for=\"answer_type\">Select Answer Type:</label><select name=\"answer_type\" id=\"answer_type\"><option value=\"short\" {% if answer_type_input == 'short' %}selected{% endif %}>Short Answer</option><option value=\"long\" {% if answer_type_input == 'long' %}selected{% endif %}>Long Answer</option></select></div><input type=\"submit\" value=\"Generate Q&A\"></form>{% if qna_results %}<div class=\"qna-results-container\"><h2>Generated Q&A Pairs:</h2>{% for item in qna_results %}<div class=\"qna-pair\"><p><strong>Question {{ loop.index }}:</strong> {{ item.question }}</p><p><strong>Answer {{ loop.index }}:</strong> {{ item.answer }}</p></div>{% endfor %}</div>{% elif request.method == 'POST' and not error %}<p style=\"text-align:center; margin-top:20px;\">No Q&A pairs were generated. The model might not have found suitable content or failed to parse its output.</p>{% endif %}</div></body></html>\n",
        "\"\"\"\n",
        "    with open(\"templates/index.html\", \"w\") as f: f.write(html_content)\n",
        "    print(\"Created/Updated templates/index.html\")\n",
        "    load_models_for_flask()\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:5000\\\"\")\n",
        "    app.run(port=5000, use_reloader=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu70W54YzfEn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dlXG0_tscqNV2TjavZma-G9w8LpqpDHe",
      "authorship_tag": "ABX9TyN04IFf4c7D2gWwrJuh5B+O",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a820ef391d34fcb8dcfee3088e82883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1de8b49b8f55484fba3517973556ef9d",
              "IPY_MODEL_7d7c5c16b0e4460c9c79138124e1a133",
              "IPY_MODEL_6f655f6df065482c893eae3e808ba49d"
            ],
            "layout": "IPY_MODEL_976627ab2fab444680626b61e633631b"
          }
        },
        "1de8b49b8f55484fba3517973556ef9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23e9f9078b864e31a17913100f2c2a3c",
            "placeholder": "​",
            "style": "IPY_MODEL_3587453e3a1c44d1a9f96b91f9e662a0",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7d7c5c16b0e4460c9c79138124e1a133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f72882b66eb94d44a1240cfd5a082ead",
            "max": 2324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_feacbf9718d04deea65ce483c7b73313",
            "value": 2324
          }
        },
        "6f655f6df065482c893eae3e808ba49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c071e9979be64a1096cd413a05436a42",
            "placeholder": "​",
            "style": "IPY_MODEL_4f942e39605a41eb91812700377a7453",
            "value": " 2.32k/2.32k [00:00&lt;00:00, 117kB/s]"
          }
        },
        "976627ab2fab444680626b61e633631b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23e9f9078b864e31a17913100f2c2a3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3587453e3a1c44d1a9f96b91f9e662a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f72882b66eb94d44a1240cfd5a082ead": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feacbf9718d04deea65ce483c7b73313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c071e9979be64a1096cd413a05436a42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f942e39605a41eb91812700377a7453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea6b9d9a1a6642f7bea46d0f82a43bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_276173b7e4f74cdc817d93ec2c645dcc",
              "IPY_MODEL_cebebd6b6b514bf7a873de56f15bbbe9",
              "IPY_MODEL_4eff33247549424f910000f4e36815fd"
            ],
            "layout": "IPY_MODEL_f84e1de437e844fda23a263aaeebc5bc"
          }
        },
        "276173b7e4f74cdc817d93ec2c645dcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3835051c52424e2fb56024a810ffba4b",
            "placeholder": "​",
            "style": "IPY_MODEL_e481f94cb5a748eb918649e90d58e551",
            "value": "spiece.model: 100%"
          }
        },
        "cebebd6b6b514bf7a873de56f15bbbe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_017a5ef2a93f457a890f416cbd7a19a3",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d2cbc0a75cb4829ac897395f8d21c29",
            "value": 791656
          }
        },
        "4eff33247549424f910000f4e36815fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_461c141a38e54eecbb7d9d12c2d0f405",
            "placeholder": "​",
            "style": "IPY_MODEL_39fe474bdc404ac68ccc9258772ae154",
            "value": " 792k/792k [00:00&lt;00:00, 15.9MB/s]"
          }
        },
        "f84e1de437e844fda23a263aaeebc5bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3835051c52424e2fb56024a810ffba4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e481f94cb5a748eb918649e90d58e551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "017a5ef2a93f457a890f416cbd7a19a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d2cbc0a75cb4829ac897395f8d21c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "461c141a38e54eecbb7d9d12c2d0f405": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39fe474bdc404ac68ccc9258772ae154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9721a5b313924ca7981a80fc125e4b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ac18e271a324b5fbffd8bb07ac5c904",
              "IPY_MODEL_70a8ccb685b545489ba0b9d4efe48206",
              "IPY_MODEL_936e813dd3e0453f80d290d6b054cd72"
            ],
            "layout": "IPY_MODEL_5f25d0a8accb41d1a0f0afe3aed8b5ea"
          }
        },
        "0ac18e271a324b5fbffd8bb07ac5c904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73eb0c718b97402e9c96e5eec12b6d72",
            "placeholder": "​",
            "style": "IPY_MODEL_a7a9da46caa5490aa97c3777131eb396",
            "value": "tokenizer.json: 100%"
          }
        },
        "70a8ccb685b545489ba0b9d4efe48206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd040891b64046cb9c8d7f8596ee74c9",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c9fc0fac02e4527a11b40f62184556e",
            "value": 1389353
          }
        },
        "936e813dd3e0453f80d290d6b054cd72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_415b7bbc42354d14938de3c0ccbf60a4",
            "placeholder": "​",
            "style": "IPY_MODEL_391d67bb59754806b222a8e489a1f24f",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 19.0MB/s]"
          }
        },
        "5f25d0a8accb41d1a0f0afe3aed8b5ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73eb0c718b97402e9c96e5eec12b6d72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7a9da46caa5490aa97c3777131eb396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd040891b64046cb9c8d7f8596ee74c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c9fc0fac02e4527a11b40f62184556e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "415b7bbc42354d14938de3c0ccbf60a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "391d67bb59754806b222a8e489a1f24f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "211d25f6a7e14c55a0a2865c4151d1e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2a1fd03d11c4b87becc4d86a8095360",
              "IPY_MODEL_372b801bcfb64ec29eae1fe24a8fd506",
              "IPY_MODEL_5046fb44028a46ffa7613e5f10395ad1"
            ],
            "layout": "IPY_MODEL_68943090b2ba4edf967033fddbcbe5d2"
          }
        },
        "c2a1fd03d11c4b87becc4d86a8095360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afb9d58e654d484f97c0ec72c162fb43",
            "placeholder": "​",
            "style": "IPY_MODEL_c1fa103e12684cf78a13b9696dd4231d",
            "value": "Map: 100%"
          }
        },
        "372b801bcfb64ec29eae1fe24a8fd506": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9277507201a446dfaebe045a38e1003e",
            "max": 2394,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67c97cd555844661970124bbb7542f96",
            "value": 2394
          }
        },
        "5046fb44028a46ffa7613e5f10395ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aac881c7e0544954a20d768c7596bd90",
            "placeholder": "​",
            "style": "IPY_MODEL_644b4fbf682d4b3f9ff154a01f30ac1e",
            "value": " 2394/2394 [00:05&lt;00:00, 438.43 examples/s]"
          }
        },
        "68943090b2ba4edf967033fddbcbe5d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afb9d58e654d484f97c0ec72c162fb43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1fa103e12684cf78a13b9696dd4231d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9277507201a446dfaebe045a38e1003e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67c97cd555844661970124bbb7542f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aac881c7e0544954a20d768c7596bd90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "644b4fbf682d4b3f9ff154a01f30ac1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3a701397af44d60b8932f40b99c0e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20b0336c6f0445f09d1e0d6d8a1a5277",
              "IPY_MODEL_3642ce5a0b9f481bb3cc9b73692ee58c",
              "IPY_MODEL_d647235d518d4e028d5ef037384c6741"
            ],
            "layout": "IPY_MODEL_dad0a09daa4d410da5d1d5c2f7effbcd"
          }
        },
        "20b0336c6f0445f09d1e0d6d8a1a5277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_405aee25802f427db2d1ecd34f7594de",
            "placeholder": "​",
            "style": "IPY_MODEL_a81c6cc028264553a601a182b59dab9a",
            "value": "Map: 100%"
          }
        },
        "3642ce5a0b9f481bb3cc9b73692ee58c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aed787c70cf644e5bba50555a9b0f266",
            "max": 587,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84e6007ae3d44f3c8ebbaab82af657dd",
            "value": 587
          }
        },
        "d647235d518d4e028d5ef037384c6741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c24bf4e5421a40a4adf68a049f3f6451",
            "placeholder": "​",
            "style": "IPY_MODEL_286e3f1cf09d496c81457b21645f3b00",
            "value": " 587/587 [00:00&lt;00:00, 671.12 examples/s]"
          }
        },
        "dad0a09daa4d410da5d1d5c2f7effbcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "405aee25802f427db2d1ecd34f7594de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a81c6cc028264553a601a182b59dab9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aed787c70cf644e5bba50555a9b0f266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84e6007ae3d44f3c8ebbaab82af657dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c24bf4e5421a40a4adf68a049f3f6451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "286e3f1cf09d496c81457b21645f3b00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d55a6f7001547caae5c4423bb823e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f81759bbbb4421ca4cf8f69c36712e8",
              "IPY_MODEL_19003340b3cc48c190613475033c8965",
              "IPY_MODEL_7e7c69c9cdb643b5a734c0545e63c1fd"
            ],
            "layout": "IPY_MODEL_261211f961124c3caa97418cfc44d4bd"
          }
        },
        "3f81759bbbb4421ca4cf8f69c36712e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f35f73706d744c35b1ccb58f287b07a2",
            "placeholder": "​",
            "style": "IPY_MODEL_f63acf2c9823455d9f97f4d1132fbcdb",
            "value": "config.json: 100%"
          }
        },
        "19003340b3cc48c190613475033c8965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d73820eecdef40d295437eb995dcdf8f",
            "max": 1206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7a5f3509b354566a68711603fc961f4",
            "value": 1206
          }
        },
        "7e7c69c9cdb643b5a734c0545e63c1fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad6761d6913a45cb8a065ec367dfce52",
            "placeholder": "​",
            "style": "IPY_MODEL_bab8b5085f3548b7804f3148ba3b435e",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 46.1kB/s]"
          }
        },
        "261211f961124c3caa97418cfc44d4bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f35f73706d744c35b1ccb58f287b07a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f63acf2c9823455d9f97f4d1132fbcdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d73820eecdef40d295437eb995dcdf8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7a5f3509b354566a68711603fc961f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad6761d6913a45cb8a065ec367dfce52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bab8b5085f3548b7804f3148ba3b435e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a1a6d5e23e342e9b1684bf2da5243f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60e88e5ff9ad42e79753c04bb13d5dce",
              "IPY_MODEL_aa63f30e24c34c90ad6a653fd961a1f8",
              "IPY_MODEL_b9732fa1fd09418eba5bf68b35316cf4"
            ],
            "layout": "IPY_MODEL_6a31dd30a89e4d78beda8ee464c9afaf"
          }
        },
        "60e88e5ff9ad42e79753c04bb13d5dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59caef1e93d247a5812f605ebb3dea17",
            "placeholder": "​",
            "style": "IPY_MODEL_87507ff3023040a88ad46525f971bba7",
            "value": "model.safetensors: 100%"
          }
        },
        "aa63f30e24c34c90ad6a653fd961a1f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8d4465442d2433788f558db1c1dc0b3",
            "max": 242043056,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cba2c00ff9eb418887eab4ae31a4e02a",
            "value": 242043056
          }
        },
        "b9732fa1fd09418eba5bf68b35316cf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7711440ddf0e4d62bf5bf3f8b887e6c0",
            "placeholder": "​",
            "style": "IPY_MODEL_8e2e6509120d4c5a90b534374a001584",
            "value": " 242M/242M [00:02&lt;00:00, 118MB/s]"
          }
        },
        "6a31dd30a89e4d78beda8ee464c9afaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59caef1e93d247a5812f605ebb3dea17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87507ff3023040a88ad46525f971bba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8d4465442d2433788f558db1c1dc0b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cba2c00ff9eb418887eab4ae31a4e02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7711440ddf0e4d62bf5bf3f8b887e6c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e2e6509120d4c5a90b534374a001584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "060f7d7a130c469293f84cfb61d72961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a11c64e927434b7f9f45bdc60864f21d",
              "IPY_MODEL_fd7ae45f0e4c407bb12ed1f52c9a3f3c",
              "IPY_MODEL_6287af75745a4cef9c4cd81de69acb36"
            ],
            "layout": "IPY_MODEL_970fa21bf8c9454bb54cb99d74f2f586"
          }
        },
        "a11c64e927434b7f9f45bdc60864f21d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2eb7429af3e840fa9fe15ad8f48df7ef",
            "placeholder": "​",
            "style": "IPY_MODEL_c783533698514bb6a504bf76c4c93841",
            "value": "generation_config.json: 100%"
          }
        },
        "fd7ae45f0e4c407bb12ed1f52c9a3f3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3da3c371751e4bbbb37d93bccb25a603",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_454a39fb5255429ab32a33d29819a620",
            "value": 147
          }
        },
        "6287af75745a4cef9c4cd81de69acb36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6517d4fd19a846d58498c5c57e390744",
            "placeholder": "​",
            "style": "IPY_MODEL_992ea64b1d654907b108b66e248f82bc",
            "value": " 147/147 [00:00&lt;00:00, 5.87kB/s]"
          }
        },
        "970fa21bf8c9454bb54cb99d74f2f586": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eb7429af3e840fa9fe15ad8f48df7ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c783533698514bb6a504bf76c4c93841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3da3c371751e4bbbb37d93bccb25a603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "454a39fb5255429ab32a33d29819a620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6517d4fd19a846d58498c5c57e390744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "992ea64b1d654907b108b66e248f82bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b11f72533983469d85b8a1e99951df04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76abbc86af484289bb3699f5ba43034b",
              "IPY_MODEL_892dd14f958e408599fefb4bee1a4c25",
              "IPY_MODEL_8a1c5cf8afc84514997daee88a32db25"
            ],
            "layout": "IPY_MODEL_4ef069925de54f4eaf19cccc712fde06"
          }
        },
        "76abbc86af484289bb3699f5ba43034b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1f9a2586c49427a9546930d16b87d24",
            "placeholder": "​",
            "style": "IPY_MODEL_93e22f5bf6094242aa68f43072c934a2",
            "value": "Downloading builder script: 100%"
          }
        },
        "892dd14f958e408599fefb4bee1a4c25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_674dfaf2d3554bac8563e8f85d3756c2",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e96c91f7a93149b4a33dc0bac6dab9b4",
            "value": 6270
          }
        },
        "8a1c5cf8afc84514997daee88a32db25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b51b93bad774a14a59d07ea83f59ddd",
            "placeholder": "​",
            "style": "IPY_MODEL_5529ad81a6f64452aab94fb77602b6c6",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 168kB/s]"
          }
        },
        "4ef069925de54f4eaf19cccc712fde06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1f9a2586c49427a9546930d16b87d24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93e22f5bf6094242aa68f43072c934a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "674dfaf2d3554bac8563e8f85d3756c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e96c91f7a93149b4a33dc0bac6dab9b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b51b93bad774a14a59d07ea83f59ddd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5529ad81a6f64452aab94fb77602b6c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c13c93a95b1341c8b32a0cff935dcb94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22b479b3a9c24c819266a4ba61ae5b3c",
              "IPY_MODEL_3f4e161a42574dd78522128eed1e8caf",
              "IPY_MODEL_722d6f09645a45aa8d5d8ed9afc3cee1"
            ],
            "layout": "IPY_MODEL_4fb759d6f98449cd8837b0ca34462b8b"
          }
        },
        "22b479b3a9c24c819266a4ba61ae5b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc1c041b24a344969be871f424146bd0",
            "placeholder": "​",
            "style": "IPY_MODEL_80377b10514c4208a3e09f772e53147e",
            "value": "Map: 100%"
          }
        },
        "3f4e161a42574dd78522128eed1e8caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcfa9d1a92ce45e2bf4bc5691b8d1455",
            "max": 2413,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c8595f2c0814ff2bccc7655d403c756",
            "value": 2413
          }
        },
        "722d6f09645a45aa8d5d8ed9afc3cee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef077c9583e2467ca94207c7619b13b7",
            "placeholder": "​",
            "style": "IPY_MODEL_a306084eec5e409fb6e13146a2fabff4",
            "value": " 2413/2413 [00:07&lt;00:00, 305.30 examples/s]"
          }
        },
        "4fb759d6f98449cd8837b0ca34462b8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc1c041b24a344969be871f424146bd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80377b10514c4208a3e09f772e53147e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcfa9d1a92ce45e2bf4bc5691b8d1455": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c8595f2c0814ff2bccc7655d403c756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef077c9583e2467ca94207c7619b13b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a306084eec5e409fb6e13146a2fabff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "061b792354d04c42b34d080d86e3c4ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_701c82ade9144b2ab71aaaef0ba0cbdc",
              "IPY_MODEL_65f7ef776d6f4bf9b767f600e4ae4744",
              "IPY_MODEL_8e7ee6dc4d7845ce9713844fe8a31240"
            ],
            "layout": "IPY_MODEL_352c708a458e4bc3a00e3db564856799"
          }
        },
        "701c82ade9144b2ab71aaaef0ba0cbdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c537a4eac3e243b98b286ebd8e2f86e3",
            "placeholder": "​",
            "style": "IPY_MODEL_877428dd8381472f9a46b67c23c09d7e",
            "value": "Map: 100%"
          }
        },
        "65f7ef776d6f4bf9b767f600e4ae4744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d03cb1e6dcc4c99987092a9df2658f5",
            "max": 578,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ef8d9fae221439aa32a604904d8b2e9",
            "value": 578
          }
        },
        "8e7ee6dc4d7845ce9713844fe8a31240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8596f8bbc80b48c3a79742852a35c511",
            "placeholder": "​",
            "style": "IPY_MODEL_a4bc0ceb54634d1b81adf07164032189",
            "value": " 578/578 [00:01&lt;00:00, 429.79 examples/s]"
          }
        },
        "352c708a458e4bc3a00e3db564856799": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c537a4eac3e243b98b286ebd8e2f86e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "877428dd8381472f9a46b67c23c09d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d03cb1e6dcc4c99987092a9df2658f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ef8d9fae221439aa32a604904d8b2e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8596f8bbc80b48c3a79742852a35c511": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4bc0ceb54634d1b81adf07164032189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}